import os
import logging
import json
import requests
from pathlib import Path
from typing import List, Dict, Optional, Union, Any
import torch
import faiss
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from dotenv import load_dotenv
import numpy as np
import huggingface_hub
import psutil  # A√±adido para verificar la memoria disponible
import re

# Crear directorio de logs si no existe
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

# Lista de palabras de saludo
greeting_words = ['hola', 'buenas', 'buen d√≠a', 'buen dia', 'buenos d√≠as', 'buenos dias', 
                'buenas tardes', 'buenas noches', 'saludos', 'que tal', 'qu√© tal', 'como va', 'c√≥mo va']

# Lista de emojis para enriquecer las respuestas
information_emojis = ["üìö", "üìñ", "‚ÑπÔ∏è", "üìä", "üîç", "üìù", "üìã", "üìà", "üìå", "üß†"]
greeting_emojis = ["üëã", "üòä", "ü§ì", "üë®‚Äç‚öïÔ∏è", "üë©‚Äç‚öïÔ∏è", "üéì", "üåü"]

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Salida a consola
        logging.FileHandler(log_dir / 'app.log')  # Salida a archivo
    ]
)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv()

# Configurar token de Hugging Face si est√° disponible
if "HUGGING_FACE_HUB_TOKEN" in os.environ:
    huggingface_hub.login(token=os.environ["HUGGING_FACE_HUB_TOKEN"], add_to_git_credential=False)
    logger.info("Configurado token de Hugging Face desde variables de entorno")

# Determinar el entorno (desarrollo o producci√≥n)
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
logger.info(f"Iniciando sistema RAG en entorno: {ENVIRONMENT}")

# Funci√≥n para configurar el dispositivo seg√∫n preferencias
def get_device():
    """Configura el dispositivo seg√∫n preferencias y disponibilidad."""
    device_pref = os.getenv('DEVICE', 'auto')
    
    if device_pref == 'auto':
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    else:
        device = device_pref  # Usa espec√≠ficamente cuda, cpu o mps si se especifica
    
    logger.info(f"Usando dispositivo: {device}")
    return device

# Importaciones condicionales para Pinecone (solo en producci√≥n)
if ENVIRONMENT == 'production':
    try:
        import pinecone
        PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
        PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')
        PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME', 'uba-chatbot-embeddings')
        if not all([PINECONE_API_KEY, PINECONE_ENVIRONMENT]):
            logger.warning("Falta configuraci√≥n de Pinecone. Se usar√° FAISS local.")
            ENVIRONMENT = 'development'
        else:
            logger.info("Usando Pinecone para b√∫squeda de embeddings en producci√≥n.")
            # Inicializar pinecone
            pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)
    except ImportError:
        logger.warning("No se pudo importar pinecone. Se usar√° FAISS local.")
        ENVIRONMENT = 'development'

# Clase base abstracta para b√∫squedas vectoriales
class VectorStore:
    def search(self, query_embedding: List[float], k: int) -> List[Dict]:
        """B√∫squeda de vectores similares"""
        raise NotImplementedError("Este m√©todo debe ser implementado por las subclases")

# Implementaci√≥n para FAISS (desarrollo)
class FAISSVectorStore(VectorStore):
    def __init__(self, index_path: str, metadata_path: str):
        """
        Inicializa el almac√©n vectorial FAISS.
        
        Args:
            index_path (str): Ruta al archivo de √≠ndice FAISS
            metadata_path (str): Ruta al archivo de metadatos
        """
        if not os.path.exists(index_path):
            raise FileNotFoundError(f"No se encontr√≥ el √≠ndice FAISS en {index_path}")
            
        self.index = faiss.read_index(index_path)
        self.metadata = pd.read_csv(metadata_path)
        logger.info(f"√çndice FAISS cargado con {self.index.ntotal} vectores")
    
        # Umbral de similitud m√≠nimo (ajustable seg√∫n necesidad)
        self.similarity_threshold = 0.1  # Reducido de 0.6 a 0.1 para ser m√°s permisivo
    
    def search(self, query_embedding: List[float], k: int = 5) -> List[Dict]:
        """
        B√∫squeda de vectores similares en FAISS con mejoras.
        
        Args:
            query_embedding (List[float]): Embedding de la consulta
            k (int): N√∫mero de resultados a retornar
            
        Returns:
            List[Dict]: Lista de resultados con metadatos
        """
        # Convertir a numpy y normalizar
        query_embedding_np = np.array(query_embedding).reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding_np)
        
        # Realizar b√∫squeda
        distances, indices = self.index.search(query_embedding_np, k)
        
        # Convertir distancias L2 a similitud coseno
        similarities = np.clip(1 - distances[0] / 2, 0, 1)  # Asegurar rango [0,1]
        
        # Filtrar por umbral y ordenar por similitud
        results = []
        seen_texts = set()  # Para evitar duplicados
        
        for idx, sim in zip(indices[0], similarities):
            if idx < 0 or idx >= len(self.metadata):  # √çndice inv√°lido
                continue
            
            metadata = self.metadata.iloc[idx].to_dict()
            text = metadata.get('text', '')
            
            # Evitar duplicados y textos muy cortos
            if (text in seen_texts or 
                len(text) < 50 or 
                sim < self.similarity_threshold):
                continue
                
            seen_texts.add(text)
            metadata['similarity'] = float(sim)
            metadata['distance'] = float(distances[0][indices[0].tolist().index(idx)])
            results.append(metadata)
                
        # Ordenar por similitud
        results = sorted(results, key=lambda x: x['similarity'], reverse=True)
        
        # Logging de resultados
        if results:
            logger.info(f"Mejor similitud encontrada: {results[0]['similarity']:.3f}")
            logger.info(f"Documento m√°s relevante: {results[0].get('filename', 'N/A')}")
            logger.info(f"N√∫mero de resultados √∫nicos: {len(results)}")
        else:
            logger.warning("No se encontraron resultados que superen el umbral de similitud")
        
        return results[:k]  # Limitar a k resultados

# Implementaci√≥n para Pinecone (producci√≥n)
class PineconeVectorStore(VectorStore):
    def __init__(self, index_name: str):
        """
        Inicializa el almac√©n vectorial Pinecone.
        
        Args:
            index_name (str): Nombre del √≠ndice en Pinecone
        """
        if index_name not in pinecone.list_indexes():
            raise ValueError(f"No se encontr√≥ el √≠ndice Pinecone '{index_name}'")
            
        self.index = pinecone.Index(index_name)
        stats = self.index.describe_index_stats()
        logger.info(f"√çndice Pinecone '{index_name}' cargado con {stats['total_vector_count']} vectores")
    
    def search(self, query_embedding: List[float], k: int) -> List[Dict]:
        """
        B√∫squeda de vectores similares en Pinecone.
        
        Args:
            query_embedding (List[float]): Embedding de la consulta
            k (int): N√∫mero de resultados a retornar
            
        Returns:
            List[Dict]: Lista de resultados con metadatos
        """
        # Realizar b√∫squeda en Pinecone
        query_results = self.index.query(
            vector=query_embedding,
            top_k=k,
            include_metadata=True
        )
        
        # Formatear resultados
        results = []
        for match in query_results['matches']:
            # Extraer metadatos
            metadata = match['metadata']
            metadata['distance'] = 1.0 - match['score']  # Convertir similitud coseno a distancia
            results.append(metadata)
                
        return results

def get_available_memory_gb():
    """
    Obtiene la cantidad de memoria RAM disponible en GB.
    
    Returns:
        float: Memoria disponible en GB
    """
    try:
        # Intentar obtener la memoria virtual disponible
        available = psutil.virtual_memory().available
        available_gb = available / (1024 ** 3)  # Convertir a GB
        logger.info(f"Memoria disponible: {available_gb:.2f} GB")
        return available_gb
    except Exception as e:
        logger.warning(f"Error al obtener memoria disponible: {str(e)}")
        # En caso de error, devolver un valor que permita cargar el modelo grande
        # para no cambiar el comportamiento previo
        return 32.0

def load_model_with_fallback(model_path: str, load_kwargs: Dict) -> tuple:
    """
    Intenta cargar un modelo y, si falla por autenticaci√≥n o memoria insuficiente, 
    usa un modelo abierto como alternativa.
    Soporta optimizaciones seg√∫n plataforma.
    
    Args:
        model_path (str): Ruta al modelo preferido
        load_kwargs (Dict): Argumentos para cargar el modelo
        
    Returns:
        tuple: (modelo, tokenizer, nombre_del_modelo_cargado)
    """
    # Modelo abierto de respaldo
    fallback_model = os.getenv('FALLBACK_MODEL_NAME', 'google/gemma-2b')
    
    # Detectar si estamos en Mac con Apple Silicon
    is_mac_silicon = torch.backends.mps.is_available()
    
    # Verificar memoria disponible
    available_memory_gb = get_available_memory_gb()
    memory_threshold_gb = 16.0  # Necesitamos al menos 16GB libres para Mistral 7B
    
    # Decidir si usar el modelo de respaldo basado en la memoria disponible
    if available_memory_gb < memory_threshold_gb:
        logger.warning(f"Memoria disponible ({available_memory_gb:.2f} GB) es menor que el umbral recomendado ({memory_threshold_gb} GB)")
        logger.warning(f"Usando modelo de respaldo para evitar errores de memoria: {fallback_model}")
        model_path = fallback_model
    
    # Ajustar configuraci√≥n seg√∫n la plataforma
    if is_mac_silicon:
        logger.info("Detectado Mac con Apple Silicon, ajustando configuraci√≥n de carga")
        
        # Para Mac Silicon, desactivamos bitsandbytes independientemente de la configuraci√≥n
        # Quitamos la cuantizaci√≥n que causa problemas en Mac
        if "load_in_8bit" in load_kwargs and load_kwargs["load_in_8bit"]:
            logger.warning("Desactivando cuantizaci√≥n de 8 bits en Apple Silicon")
            load_kwargs["load_in_8bit"] = False
        if "load_in_4bit" in load_kwargs and load_kwargs["load_in_4bit"]:
            logger.warning("Desactivando cuantizaci√≥n de 4 bits en Apple Silicon")
            load_kwargs["load_in_4bit"] = False
        
        # Configurar para usar la aceleraci√≥n de MPS por defecto
        if "device_map" in load_kwargs:
            load_kwargs["device_map"] = "mps"
        
        # Aumentamos la precisi√≥n para compensar la falta de cuantizaci√≥n
        load_kwargs["torch_dtype"] = torch.float16
    else:
        # En otras plataformas (Windows/Linux), mantenemos la configuraci√≥n original
        # que puede incluir bitsandbytes si est√° activado
        logger.info("Usando configuraci√≥n est√°ndar para plataforma no-Mac")
        
        # Verificar si se ha habilitado 8bit o 4bit
        use_8bit = os.getenv('USE_8BIT', 'False').lower() == 'true'
        use_4bit = os.getenv('USE_4BIT', 'False').lower() == 'true'
        
        # Aplicar configuraci√≥n de bitsandbytes si est√° activada
        if use_8bit:
            logger.info("Activando cuantizaci√≥n de 8 bits con bitsandbytes")
            load_kwargs["load_in_8bit"] = True
        elif use_4bit:
            logger.info("Activando cuantizaci√≥n de 4 bits con bitsandbytes")
            load_kwargs["load_in_4bit"] = True
    
    try:
        logger.info(f"Intentando cargar modelo con PyTorch: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path, **load_kwargs)
        logger.info(f"Modelo cargado exitosamente con PyTorch: {model_path}")
        return model, tokenizer, model_path
    except Exception as e:
        error_str = str(e)
        if "Access to model" in error_str and "is restricted" in error_str:
            logger.warning(f"ACCESO RESTRINGIDO: El modelo {model_path} requiere autenticaci√≥n en Hugging Face.")
            logger.warning("Para usar este modelo, debes ejecutar 'huggingface-cli login' e ingresar tu token.")
            logger.warning(f"Cambiando autom√°ticamente al modelo abierto: {fallback_model}")
            
            # Cargar modelo alternativo
            tokenizer = AutoTokenizer.from_pretrained(fallback_model)
            model = AutoModelForCausalLM.from_pretrained(fallback_model, **load_kwargs)
            return model, tokenizer, fallback_model
        else:
            # Si es otro tipo de error, reenviar la excepci√≥n
            logger.error(f"Error al cargar el modelo: {error_str}")
            raise

# Clase base para modelos
class BaseModel:
    def generate(self, prompt: str, **kwargs) -> str:
        raise NotImplementedError

# Clase para usar la API de Hugging Face
class APIModel(BaseModel):
    def __init__(self, model_name: str, api_token: str, timeout: int = 30):
        """
        Inicializa el cliente de la API de Hugging Face.
        
        Args:
            model_name (str): Nombre del modelo en Hugging Face
            api_token (str): Token de la API de Hugging Face
            timeout (int): Timeout para las llamadas a la API
        """
        self.model_name = model_name
        self.api_url = f"https://api-inference.huggingface.co/models/{model_name}"
        self.headers = {"Authorization": f"Bearer {api_token}"}
        self.timeout = timeout
        
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Genera texto usando la API de Hugging Face.
        
        Args:
            prompt (str): Texto de entrada
            kwargs: Argumentos adicionales para la generaci√≥n
            
        Returns:
            str: Texto generado
        """
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": kwargs.get("max_length", 512),
                "temperature": kwargs.get("temperature", 0.7),
                "top_p": kwargs.get("top_p", 0.9),
                "top_k": kwargs.get("top_k", 50),
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(
                self.api_url, 
                headers=self.headers, 
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            result = response.json()
            
            if isinstance(result, list) and len(result) > 0:
                generated_text = result[0].get("generated_text", "")
                return generated_text.strip()
            else:
                raise ValueError(f"Respuesta inesperada de la API: {result}")
                
        except Exception as e:
            logger.error(f"Error al generar texto con la API de {self.model_name}: {str(e)}")
            raise

# Clase para modelo local
class LocalModel(BaseModel):
    def __init__(self, model, tokenizer, device):
        """
        Inicializa el modelo local.
        
        Args:
            model: Modelo de transformers
            tokenizer: Tokenizer del modelo
            device: Dispositivo para inferencia
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Genera texto usando el modelo local.
        
        Args:
            prompt (str): Texto de entrada
            kwargs: Argumentos adicionales para la generaci√≥n
            
        Returns:
            str: Texto generado
        """
        try:
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=len(inputs[0]) + kwargs.get("max_length", 512),
                    temperature=kwargs.get("temperature", 0.7),
                    top_p=kwargs.get("top_p", 0.9),
                    top_k=kwargs.get("top_k", 50),
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
        except Exception as e:
            logger.error(f"Error al generar texto localmente: {str(e)}")
            raise

class RAGSystem:
    def __init__(
        self,
        model_path: str = os.getenv('BASE_MODEL_PATH', 'models/finetuned_model'),
        embeddings_dir: str = os.getenv('EMBEDDINGS_DIR', 'data/embeddings'),
        device: str = os.getenv('DEVICE', 'mps')
    ):
        """
        Inicializa el sistema RAG con la siguiente l√≥gica de prioridad:
        1. Si USE_API=False:
           - Intenta cargar modelo fine-tuneado local
           - Si falla por memoria o no existe, usa API como fallback
        2. Si USE_API=True:
           - Usa directamente la API
        """
        self.device = device
        self.embeddings_dir = Path(embeddings_dir)
        
        # Obtener configuraci√≥n
        self.use_api = os.getenv('USE_API', 'True').lower() == 'true'
        self.api_token = os.getenv('HUGGING_FACE_HUB_TOKEN')
        self.base_model = os.getenv('BASE_MODEL_NAME')
        self.fallback_model = os.getenv('FALLBACK_MODEL_NAME')
        
        # Verificar si existe modelo fine-tuneado
        model_exists = os.path.exists(model_path) and os.path.isdir(model_path)
        
        if not self.use_api and model_exists:
            # Intentar usar modelo local primero
            try:
                available_memory = get_available_memory_gb()
                if available_memory < 16:
                    raise MemoryError(f"Memoria insuficiente ({available_memory:.2f} GB) para cargar modelo local")
                
                logger.info(f"Intentando cargar modelo fine-tuneado local desde: {model_path}")
                self.model = self._load_local_model(model_path)
                logger.info("Modelo local cargado exitosamente")
                return
            except Exception as e:
                logger.warning(f"No se pudo cargar el modelo local: {str(e)}")
                logger.info("Cambiando a API como fallback")
        
        # Si llegamos aqu√≠, usamos la API (ya sea por configuraci√≥n o como fallback)
        try:
            logger.info("Inicializando modelo via API")
            self.model = self._initialize_api_model()
        except Exception as e:
            raise RuntimeError(f"No se pudo inicializar ning√∫n modelo (local ni API): {str(e)}")
        
        # Cargar modelo de embeddings
        embedding_model_name = 'hiiamsid/sentence_similarity_spanish_es'  # Modelo fijo para coincidir con el √≠ndice
        try:
            logger.info(f"Intentando cargar modelo de embeddings: {embedding_model_name}")
            self.embedding_model = SentenceTransformer(embedding_model_name)
            logger.info(f"Modelo de embeddings cargado: {embedding_model_name}")
            logger.info(f"Dimensi√≥n del modelo: {self.embedding_model.get_sentence_embedding_dimension()}")
        except Exception as e:
            logger.warning(f"Error al cargar modelo de embeddings principal: {str(e)}")
            logger.info("Usando modelo de respaldo con la misma dimensionalidad")
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
            logger.info(f"Dimensi√≥n del modelo de respaldo: {self.embedding_model.get_sentence_embedding_dimension()}")
        
        # Inicializar el almac√©n vectorial
        self.vector_store = self._initialize_vector_store()

        # Configurar umbral de similitud
        self.similarity_threshold = float(os.getenv('SIMILARITY_THRESHOLD', '0.1'))  # Umbral m√°s permisivo
        
    def _load_local_model(self, model_path: str):
        """Carga el modelo local con la configuraci√≥n apropiada"""
        load_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": "auto"
        }
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **load_kwargs
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        return LocalModel(model, tokenizer, self.device)

    def _initialize_api_model(self) -> APIModel:
        """Inicializa el modelo de API con fallback"""
        if not self.api_token:
            raise ValueError("Se requiere HUGGING_FACE_HUB_TOKEN para usar la API")
            
        try:
            # Intentar primero con Mistral
            model = APIModel(self.base_model, self.api_token)
            # Verificar que funciona
            model.generate("Test", max_length=10)
            logger.info(f"Usando modelo principal via API: {self.base_model}")
            return model
        except Exception as e:
            logger.warning(f"Error con modelo principal: {str(e)}")
            logger.info(f"Intentando con modelo de fallback: {self.fallback_model}")
            
            try:
                # Fallback a TinyLlama
                model = APIModel(self.fallback_model, self.api_token)
                # Verificar que funciona
                model.generate("Test", max_length=10)
                logger.info(f"Usando modelo de fallback via API: {self.fallback_model}")
                return model
            except Exception as e:
                raise RuntimeError(f"No se pudo inicializar ning√∫n modelo: {str(e)}")

    def _initialize_vector_store(self) -> VectorStore:
        """
        Inicializa el almac√©n vectorial adecuado seg√∫n el entorno.
        
        Returns:
            VectorStore: Implementaci√≥n del almac√©n vectorial
        """
        if ENVIRONMENT == 'production':
            # Usar Pinecone en producci√≥n
            try:
                return PineconeVectorStore(PINECONE_INDEX_NAME)
            except (ValueError, NameError) as e:
                logger.error(f"Error al inicializar Pinecone: {str(e)}")
                logger.warning("Fallback a FAISS local")
                # Fallback a FAISS si hay error
        
        # Usar FAISS en desarrollo o como fallback
        index_path = str(self.embeddings_dir / 'faiss_index.bin')
        metadata_path = str(self.embeddings_dir / 'metadata.csv')
        return FAISSVectorStore(index_path, metadata_path)
        
    def _expand_query(self, query: str) -> str:
        """
        Expande la consulta para mejorar la b√∫squeda sem√°ntica.
        """
        query_lower = query.lower()
        expanded_query = query
        
        # Palabras clave y sus expansiones
        keywords = {
            'sanci√≥n': ['sanci√≥n', 'sanciones', 'castigo', 'penalidad', 'disciplina', 'r√©gimen disciplinario'],
            'agredir': ['agredir', 'agresi√≥n', 'violencia', 'ataque', 'golpear'],
            'profesor': ['profesor', 'docente', 'maestro', 'autoridad universitaria'],
            'alumno': ['alumno', 'estudiante', 'cursante'],
            'suspensi√≥n': ['suspensi√≥n', 'expulsi√≥n', 'separaci√≥n'],
            'f√≠sicamente': ['f√≠sicamente', 'f√≠sico', 'corporal', 'material']
        }
        
        # Buscar palabras clave en la consulta
        for key, expansions in keywords.items():
            if any(word in query_lower for word in expansions):
                expanded_query = f"{expanded_query} {' '.join(expansions)}"
        
        # Agregar referencias a art√≠culos relevantes
        if any(word in query_lower for word in keywords['sanci√≥n'] + keywords['agredir']):
            expanded_query = f"{expanded_query} art√≠culo 13 art√≠culo 14 art√≠culo 15 r√©gimen disciplinario"
        
        logger.info(f"Consulta expandida: {expanded_query}")
        return expanded_query
        
    def retrieve_relevant_chunks(self, query: str, k: int = None) -> List[Dict]:
        """
        Recupera chunks relevantes para una consulta.
        
        Args:
            query (str): Consulta del usuario
            k (int): N√∫mero de chunks a recuperar
            
        Returns:
            List[Dict]: Lista de chunks relevantes con metadatos
        """
        if k is None:
            k = int(os.getenv('RAG_NUM_CHUNKS', 5))
            
        # Preprocesar la consulta
        query = query.strip()
        if not query:
            return []
            
        # Generar embedding de la consulta
        try:
            # Expandir la consulta para mejorar la b√∫squeda
            expanded_query = self._expand_query(query)
            logger.info(f"Consulta expandida: {expanded_query}")
            
            query_embedding = self.embedding_model.encode(
                [expanded_query],
                convert_to_numpy=True,
                normalize_embeddings=True
            )[0]
            logger.info(f"Embedding generado con forma: {query_embedding.shape}")
        except Exception as e:
            logger.error(f"Error al generar embedding de consulta: {str(e)}", exc_info=True)
            return []
            
        # Realizar b√∫squeda
        try:
            # Verificar que el vector_store est√° inicializado
            if not hasattr(self, 'vector_store'):
                logger.error("vector_store no est√° inicializado")
                return []
                
            logger.info("Iniciando b√∫squeda en vector_store")
            results = self.vector_store.search(query_embedding, k=k*2)
            logger.info(f"B√∫squeda completada, resultados obtenidos: {len(results)}")
            
            # Verificar si los resultados son relevantes
            if not results:
                logger.warning("No se encontraron chunks relevantes para la consulta.")
                
                # Verificar si la consulta es sobre tr√°mites comunes
                query_lower = query.lower()
                tramites_keywords = {
                    'constancia': 'a) Constancia de Alumno Regular',
                    'alumno regular': 'a) Constancia de Alumno Regular',
                    'baja': 'b) Baja de Materias',
                    'dar de baja': 'b) Baja de Materias',
                    'reincorporaci√≥n': 'c) Reincorporaci√≥n',
                    'reincorporacion': 'c) Reincorporaci√≥n',
                    'recursada': 'd) Recursadas',
                    'recursar': 'd) Recursadas'
                }
                
                # Buscar coincidencias en las palabras clave
                tramite_encontrado = None
                for keyword, tramite in tramites_keywords.items():
                    if keyword in query_lower:
                        tramite_encontrado = tramite
                    break
            
                if tramite_encontrado:
                    # Usar la informaci√≥n base del tr√°mite correspondiente
                    if 'constancia' in query_lower or 'alumno regular' in query_lower:
                        response = """Para tramitar la constancia de alumno regular:
1. **Tramitar en el Sitio de Inscripciones**
2. Ingresar con DNI y contrase√±a
3. Seleccionar "Constancia de alumno regular"
4. Imprimir y presentar en ventanilla del Ciclo Biom√©dico con Libreta/DNI"""
                    elif 'baja' in query_lower:
                        response = """Para dar de baja una materia:
- **Plazo m√°ximo**: 2 semanas antes del primer parcial o hasta el 25% de la cursada
- **Pasos**:
  1. Tramitar en Sitio de Inscripciones
  2. Seleccionar "Baja de asignatura"
  3. No requiere presentaci√≥n en ventanilla si el estado es "Resuelto Positivamente" """
                    elif 'reincorporaci√≥n' in query_lower or 'reincorporacion' in query_lower:
                        response = """Para solicitar la reincorporaci√≥n:
- **Primera reincorporaci√≥n**: 
  - Tr√°mite autom√°tico en el sistema
  - No requiere presentaci√≥n en ventanilla
- **Segunda reincorporaci√≥n o m√°s**:
  1. Tramitar en Sitio de Inscripciones
  2. Presentar documentaci√≥n en ventanilla
  3. La Comisi√≥n de Readmisi√≥n evaluar√° el caso"""
                    else:  # recursada
                        response = """Para solicitar una recursada:
- **Si figura como BAJA en cursada anterior**:
  - Sin arancel
  - Inscripci√≥n normal como primera vez
- **Si NO figura como BAJA**:
  1. Generar tr√°mite
  2. Pagar arancel en Tesorer√≠a
  3. Presentar comprobante en buzones del Ciclo Biom√©dico"""
                    
                    return {
                        "query": query,
                        "response": response,
                        "relevant_chunks": [],
                        "sources": ["Informaci√≥n de Tr√°mites Comunes"]
                    }
                else:
                    # Si no es un tr√°mite com√∫n, usar respuesta est√°ndar con derivaci√≥n por email
                    has_greeting = any(word in query.lower() for word in greeting_words)
                    if has_greeting:
                        standard_no_info_response = f"üë®‚Äç‚öïÔ∏è ¬°Hola! Soy DrCecim. Lo siento, no tengo informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas. Si tienes otras preguntas sobre temas relacionados con la Facultad de Medicina, no dudes en consultarme."
                    else:
                        standard_no_info_response = f"üë®‚Äç‚öïÔ∏è Lo siento, no tengo informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas. Si tienes otras preguntas sobre temas relacionados con la Facultad de Medicina, no dudes en consultarme."
                    
                return {
                    "query": query,
                        "response": standard_no_info_response,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Filtrar por similitud y duplicados
            filtered_results = []
            seen_content = set()
            
            for result in results:
                text = result.get('text', '').strip()
                # Crear una versi√≥n simplificada del texto para comparaci√≥n
                simple_text = ' '.join(text.lower().split())
                
                # Verificar similitud usando el campo correcto
                similarity = result.get('similarity', 0.0)
                filename = result.get('filename', '')
                
                logger.info(f"Procesando resultado - Similitud: {similarity}, Archivo: {filename}")
                
                # Dar prioridad a chunks del r√©gimen disciplinario si la consulta es sobre sanciones
                if any(word in query.lower() for word in ['sanci√≥n', 'sanciones', 'agredir', 'agresi√≥n']):
                    if "Regimen_Disciplinario.pdf" in filename:
                        # Reducir el umbral para documentos relevantes
                        if similarity >= (self.similarity_threshold * 0.5):  # Umbral m√°s permisivo para documentos relevantes
                            if simple_text not in seen_content:
                                seen_content.add(simple_text)
                                filtered_results.append(result)
                                logger.info(f"Chunk de R√©gimen Disciplinario aceptado con similitud: {similarity:.3f}")
            else:
                        # Mantener umbral normal para otros documentos
                        if similarity >= self.similarity_threshold and simple_text not in seen_content:
                            seen_content.add(simple_text)
                            filtered_results.append(result)
                            logger.info(f"Chunk de otro documento aceptado con similitud: {similarity:.3f}")
                        else:
                            # Para otras consultas, usar el umbral normal
                            if similarity >= self.similarity_threshold and simple_text not in seen_content:
                                seen_content.add(simple_text)
                                filtered_results.append(result)
                                logger.info(f"Chunk aceptado con similitud: {similarity:.3f}")
            
            # Ordenar por similitud y limitar a k resultados
            filtered_results = sorted(filtered_results, key=lambda x: x.get('similarity', 0.0), reverse=True)[:k]
            
            # Logging de resultados
            logger.info(f"Recuperados {len(filtered_results)} chunks √∫nicos de {len(results)} totales")
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"Error en la b√∫squeda de chunks: {str(e)}", exc_info=True)
            return []
        
    def generate_response(self, query: str, context: str, sources: List[str] = None) -> str:
        """
        Genera una respuesta basada en el contexto y la consulta.
        
        Args:
            query (str): Consulta del usuario
            context (str): Contexto relevante recuperado
            sources (List[str]): Lista de fuentes de informaci√≥n
            
        Returns:
            str: Respuesta generada
        """
        # Prompt base para el asistente administrativo
        system_prompt = """Eres un asistente administrativo especializado de la Universidad de Buenos Aires (UBA).
Tu rol es proporcionar informaci√≥n precisa sobre tr√°mites y procedimientos administrativos.

INSTRUCCIONES IMPORTANTES:
1. Responde SOLO con informaci√≥n verificada que encuentres en el contexto proporcionado
2. Si no tienes informaci√≥n suficiente en el contexto, sigue estas pautas:

A) Para tr√°mites comunes conocidos, proporciona esta informaci√≥n base:

PREGUNTAS FRECUENTES Y RESPUESTAS EST√ÅNDAR:

1. CONSTANCIAS Y CERTIFICADOS:
   - "¬øD√≥nde puedo tramitar la constancia de alumno regular?"
   Respuesta base:
   - **Paso 1:** Ingresar DNI y contrase√±a en Sitio de Inscripciones
   - **Paso 2:** Seleccionar "Constancia de alumno regular"
   - **Paso 3:** Imprimir formulario (1 hoja con 3 certificados)
   - **Paso 4:** Presentar en ventanilla del Ciclo Biom√©dico con Libreta Universitaria o DNI

2. BAJAS Y MODIFICACIONES:
   a) "¬øCu√°nto tiempo tengo para dar de baja una materia?"
   Respuesta base:
   - Plazo m√°ximo: **2 semanas antes del primer parcial**
   - O **hasta el 25% de la cursada** en materias sin parcial
   - **Paso 1:** Ingresar al Sitio de Inscripciones
   - **Paso 2:** Seleccionar "Baja de asignatura"
   - **Paso 3:** Si aparece "Resuelto Positivamente", no requiere m√°s tr√°mites

   b) "¬øC√≥mo anulo una inscripci√≥n a final?"
   Respuesta base:
   - Acudir a ventanilla del Ciclo Biom√©dico
   - Presentar n√∫mero de constancia del tr√°mite de inscripci√≥n
   
   c) "¬øQu√© hago si no logro inscribirme o no salgo asignado?"
   Respuesta base:
   - Dirigirse a la c√°tedra o departamento correspondiente
   - Solicitar inclusi√≥n en lista
   - Presentar Libreta Universitaria o DNI

3. REINCORPORACIONES:
   - "¬øC√≥mo solicito la reincorporaci√≥n a la Carrera?"
   Respuesta base:
   - **Primera reincorporaci√≥n:**
     * Tr√°mite autom√°tico en sistema
     * No requiere presentaci√≥n en ventanilla
   - **Segunda reincorporaci√≥n o m√°s:**
     * Realizar tr√°mite en Sitio de Inscripciones
     * Imprimir documentaci√≥n (2 hojas: certificado y constancia)
     * Presentar en ventanilla del Ciclo Biom√©dico
     * Evaluaci√≥n por Comisi√≥n de Readmisi√≥n

4. RECURSADAS Y NUEVAS CURSADAS:
   a) "¬øC√≥mo solicito una recursada?"
   Respuesta base:
   - **Si figura como BAJA:**
     * Generar tr√°mite
     * Inscripci√≥n normal sin arancel
   - **Si NO figura como BAJA:**
     * Generar tr√°mite
     * Imprimir tal√≥n de pago
     * Pagar en Tesorer√≠a
     * Presentar comprobante en buzones

   b) "¬øC√≥mo solicito una tercera cursada?"
   Respuesta base:
   - **Paso 1:** Ingresar al Sitio de Inscripciones
   - **Paso 2:** Seleccionar "3¬∫ Cursada"
   - **Paso 3:** Imprimir constancia y certificado
   - **Si figura BAJA en cursadas anteriores:**
     * Inscripci√≥n normal sin arancel
   - **Si NO figura BAJA:**
     * Pagar arancel en Tesorer√≠a
     * Presentar comprobante en buz√≥n

   c) "¬øC√≥mo solicito una cuarta cursada?"
   Respuesta base:
   - **Paso 1:** Ingresar al Sitio de Inscripciones
   - **Paso 2:** Seleccionar "4¬∫ Cursada o m√°s"
   - **Paso 3:** Imprimir documentaci√≥n
   - **Paso 4:** Presentar en ventanilla con Libreta
   - **Paso 5:** Acudir a Direcci√≥n de Alumnos

5. PR√ìRROGAS Y EXTENSIONES:
   - "¬øC√≥mo hago el tr√°mite de pr√≥rroga de materias?"
   Respuesta base:
   - **Primera o segunda pr√≥rroga:**
     * Tramitar en Sitio de Inscripciones
     * Seleccionar "Pr√≥rroga de asignatura"
     * Tr√°mite autom√°tico resuelto positivamente
   - **Tercera pr√≥rroga o superior:**
     * Realizar tr√°mite
     * Imprimir constancia
     * Presentar en ventanilla con Libreta Universitaria

B) Para consultas sin informaci√≥n disponible:
   - Indica claramente que no tienes la informaci√≥n espec√≠fica
   - Sugiere contactar a alumnos@fmed.uba.ar para obtener informaci√≥n precisa
   - Mant√©n un tono amable y profesional al derivar la consulta

3. Mant√©n un tono profesional pero amigable
4. Estructura las respuestas en pasos claros cuando sea apropiado
5. Incluye detalles espec√≠ficos sobre documentaci√≥n requerida
6. Menciona d√≥nde debe realizarse cada tr√°mite

FORMATO DE RESPUESTA:
- Usa vi√±etas o n√∫meros para listar pasos
- Destaca informaci√≥n importante en **negrita**
- Separa secciones con l√≠neas si es necesario
- Incluye advertencias o notas importantes cuando sea relevante

Contexto proporcionado:
{context}

Consulta del usuario: {query}

Respuesta:"""

        # Preparar el prompt completo
        prompt = system_prompt.format(
            context=context,
            query=query
        )

        try:
            # Generar respuesta
            response = self.model.generate(prompt)
            
            # Si no hay informaci√≥n suficiente en el contexto
            if "no tengo informaci√≥n suficiente" in response.lower():
                return "Lo siento, no tengo informaci√≥n espec√≠fica sobre ese tr√°mite en este momento. Te sugiero consultar directamente en la ventanilla del Ciclo Biom√©dico o en la Direcci√≥n de Alumnos para obtener la informaci√≥n m√°s actualizada."
            
            # Agregar fuentes si est√°n disponibles
            if sources:
                response += "\n\nFuente(s): " + ", ".join(sources)
            
            return response
            
        except Exception as e:
            logger.error(f"Error al generar respuesta: {str(e)}")
            return "Lo siento, hubo un error al procesar tu consulta. Por favor, intenta nuevamente o consulta directamente en la Direcci√≥n de Alumnos."
        
    def process_query(self, query: str) -> Dict[str, Any]:
        """
        Procesa una consulta utilizando RAG para generar una respuesta.
        """
        try:
            # Establecer el n√∫mero de chunks por defecto
            num_chunks = int(os.getenv('RAG_NUM_CHUNKS', 3))
            logger.info(f"Procesando consulta: {query}")
            
            # Encontrar fragmentos relevantes
            relevant_chunks = self.retrieve_relevant_chunks(query, k=num_chunks)
            
            # Verificar si se encontraron chunks relevantes
            if not relevant_chunks:
                logger.warning("No se encontraron chunks relevantes para la consulta.")
                
                # Verificar si la consulta es sobre sanciones o agresiones
                query_lower = query.lower()
                if any(word in query_lower for word in ['sanci√≥n', 'sanciones', 'agredir', 'agresi√≥n']):
                    # Intentar una nueva b√∫squeda con umbral m√°s bajo para el R√©gimen Disciplinario
                    logger.info("Intentando b√∫squeda espec√≠fica en R√©gimen Disciplinario...")
                    self.similarity_threshold = 0.1  # Reducir temporalmente el umbral
                    relevant_chunks = self.retrieve_relevant_chunks(query, k=num_chunks)
                    self.similarity_threshold = float(os.getenv('SIMILARITY_THRESHOLD', 0.3))  # Restaurar umbral original
                
                if not relevant_chunks:
                    standard_no_info_response = f"üë®‚Äç‚öïÔ∏è Lo siento, no encontr√© informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas."
                
                return {
                    "query": query,
                    "response": standard_no_info_response,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Construir contexto
            context_chunks = []
            sources = []
            
            for chunk in relevant_chunks:
                if "content" in chunk and chunk["content"].strip():
                    content = chunk["content"]
                elif "text" in chunk and chunk["text"].strip():
                    content = chunk["text"]
                else:
                    continue
                
                source = ""
                if "filename" in chunk and chunk["filename"]:
                    source = os.path.basename(chunk["filename"]).replace('.pdf', '')
                    if source and source not in sources:
                        sources.append(source)
                
                formatted_chunk = f"Informaci√≥n de {source}:\n{content}"
                context_chunks.append(formatted_chunk)
                logger.info(f"Agregado chunk relevante de {source}")
            
            # Unir los chunks para formar el contexto
            context = '\n\n'.join(context_chunks)
            
            if not context.strip():
                logger.warning("No se encontr√≥ contexto suficientemente relevante")
                standard_no_info_response = f"üë®‚Äç‚öïÔ∏è Lo siento, no encontr√© informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas."
                
                return {
                    "query": query,
                    "response": standard_no_info_response,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            logger.info(f"Se encontraron {len(context_chunks)} fragmentos relevantes de {len(sources)} fuentes")
            
            # Generar respuesta
            response = self.generate_response(query, context, sources)
            
            # Agregar fuentes al final de la respuesta
            if sources:
                clean_sources = [source.replace('_', ' ').replace('-', ' ') for source in sources]
                sources_text = ", ".join(clean_sources)
                response = f"{response}\n\nEsta informaci√≥n la puedes encontrar en: {sources_text}"
            
            return {
                "query": query,
                "response": response,
                "relevant_chunks": relevant_chunks,
                "sources": sources
            }
            
        except Exception as e:
            logger.error(f"Error en process_query: {str(e)}", exc_info=True)
            error_response = f"üë®‚Äç‚öïÔ∏è Lo siento, tuve un problema procesando tu consulta. Por favor, intenta de nuevo."
            return {
                "query": query,
                "response": error_response,
                "error": str(e)
            }

def main():
    """Funci√≥n principal para ejecutar el sistema RAG."""
    # Inicializar sistema RAG
    rag = RAGSystem()
    
    # Ejemplo de uso
    while True:
        query = input("\nIngrese su consulta (o 'salir' para terminar): ")
        if query.lower() == 'salir':
            break
            
        try:
            result = rag.process_query(query)
            print("\nRespuesta:", result['response'])
            print("\nFuentes utilizadas:")
            for chunk in result['relevant_chunks']:
                if 'filename' in chunk and 'chunk_index' in chunk:
                    print(f"- {chunk['filename']} (chunk {chunk['chunk_index']})")
                else:
                    print(f"- {chunk.get('id', 'unknown')}")
        except Exception as e:
            logger.error(f"Error al procesar la consulta: {str(e)}")
            print("Lo siento, hubo un error al procesar tu consulta.")

if __name__ == "__main__":
    main() 