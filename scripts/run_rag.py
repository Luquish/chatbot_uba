import os
import logging
import json
import requests
from pathlib import Path
from typing import List, Dict, Optional, Union, Any, Tuple
import torch
import faiss
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from dotenv import load_dotenv
import numpy as np
import huggingface_hub
import psutil  # A√±adido para verificar la memoria disponible
import re
import random
from unidecode import unidecode
from sklearn.metrics.pairwise import cosine_similarity

# Crear directorio de logs si no existe
Path("logs").mkdir(exist_ok=True)

# Nueva estructura de intenciones para clasificaci√≥n sem√°ntica
INTENT_EXAMPLES = {
    'saludo': {
        'examples': [
            "hola",
            "buenos d√≠as",
            "qu√© tal",
            "buenas"
        ],
        'context': "El usuario est√° iniciando la conversaci√≥n o saludando"
    },
    'pregunta_nombre': {
        'examples': [
            "como me llamo yo",
            "cual es mi nombre",
            "sabes como me llamo",
            "como sabes mi nombre",
            "por que sabes mi nombre",
            "de donde sacaste mi nombre",
            "como conseguiste mi nombre",
            "por que conoces mi nombre"
        ],
        'context': "El usuario pregunta espec√≠ficamente sobre c√≥mo conocemos su nombre"
    },
    'cortesia': {
        'examples': [
            "c√≥mo est√°s",
            "como estas",
            "como te sentis",
            "como va",
            "todo bien"
        ],
        'context': "El usuario hace una pregunta de cortes√≠a"
    },
    'referencia_anterior': {
        'examples': [
            "res√∫meme eso",
            "resumeme eso",
            "puedes resumir lo anterior",
            "podrias resumirme el mensaje anterior",
            "podrias resumirme ese texto",
            "resume el mensaje anterior",
            "explica de nuevo",
            "expl√≠came eso",
            "explicame eso de nuevo",
            "acorta esa explicaci√≥n",
            "simplifica lo que dijiste",
            "d√≠melo m√°s corto",
            "dimelo mas corto",
            "puedes abreviar",
            "puedes hacer un resumen"
        ],
        'context': "El usuario est√° pidiendo un resumen o clarificaci√≥n del mensaje anterior"
    },
    'pregunta_capacidades': {
        'examples': [
            "qu√© pod√©s hacer",
            "en qu√© me pod√©s ayudar",
            "para qu√© serv√≠s",
            "qu√© tipo de consultas puedo hacer",
            "que sabes",
            "que sabes hacer",
            "que podes hacer",
            "que me podes decir",
            "cuales son tus funciones",
            "que funciones tenes",
            "que te puedo preguntar",
            "que puedo preguntarte",
            "que tipos de dudas puedo consultar",
            "en que me podes ayudar"
        ],
        'context': "El usuario quiere saber las capacidades del bot"
    },
    'identidad': {
        'examples': [
            "qui√©n sos",
            "c√≥mo te llam√°s",
            "sos un bot",
            "sos una persona"
        ],
        'context': "El usuario pregunta sobre la identidad del bot"
    },
    'consulta_administrativa': {
        'examples': [
            "c√≥mo hago un tr√°mite",
            "necesito una constancia",
            "d√≥nde presento documentaci√≥n",
            "quiero dar de baja una materia",
            "cu√°ntas materias debo aprobar",
            "en cu√°nto tiempo tengo que terminar la carrera",
            "c√≥mo se define el a√±o acad√©mico",
            "qu√© derechos tengo para inscribirme"
        ],
        'context': "El usuario necesita informaci√≥n sobre tr√°mites administrativos o condiciones de regularidad"
    },
    'consulta_academica': {
        'examples': [
            "cu√°ndo es el parcial",
            "d√≥nde encuentro el programa",
            "c√≥mo es la cursada",
            "qu√© necesito para aprobar",
            "c√≥mo se eval√∫a la calidad de ense√±anza",
            "qu√© materias puedo cursar"
        ],
        'context': "El usuario necesita informaci√≥n acad√©mica"
    },
    'consulta_medica': {
        'examples': [
            "me duele la cabeza",
            "tengo s√≠ntomas de",
            "d√≥nde puedo consultar por un dolor",
            "necesito un diagn√≥stico",
            "tengo fiebre"
        ],
        'context': "El usuario hace una consulta m√©dica que no podemos responder"
    },
    'consulta_reglamento': {
        'examples': [
            "qu√© dice el reglamento sobre",
            "est√° permitido",
            "cu√°les son las normas",
            "qu√© pasa si no cumplo",
            "qu√© medidas toman si me porto mal",
            "qui√©n decide las sanciones",
            "qu√© castigos hay",
            "para qu√© sirven las medidas disciplinarias",
            "qu√© sanciones aplican",
            "si cometo una falta",
            "qui√©n eval√∫a mi comportamiento",
            "qu√© pasa si rompo las reglas"
        ],
        'context': "El usuario pregunta sobre normativas, reglamentos y medidas disciplinarias"
    },
    'agradecimiento': {
        'examples': [
            "perfecto",
            "gracias",
            "ok",
            "okk",
            "okey",
            "okay",
            "dale",
            "listo",
            "entendido",
            "genial",
            "excelente",
            "b√°rbaro",
            "buen√≠simo",
            "joya"
        ],
        'context': "El usuario agradece o confirma que entendi√≥ la informaci√≥n"
    }
}

GREETING_WORDS = ['hola', 'buenos dias', 'buenas tardes', 'buenas noches', 'buen dia', 'saludos', 'que tal']

# Lista de emojis para enriquecer las respuestas
information_emojis = ["üìö", "üìñ", "‚ÑπÔ∏è", "üìä", "üîç", "üìù", "üìã", "üìà", "üìå", "üß†"]
greeting_emojis = ["üëã", "üòä", "ü§ì", "üë®‚Äç‚öïÔ∏è", "üë©‚Äç‚öïÔ∏è", "üéì", "üåü"]
warning_emojis = ["‚ö†Ô∏è", "‚ùó", "‚ö°", "üö®"]
success_emojis = ["‚úÖ", "üí´", "üéâ", "üí°"]
medical_emojis = ["üè•", "üë®‚Äç‚öïÔ∏è", "üë©‚Äç‚öïÔ∏è", "ü©∫"]

# Palabras clave para expansi√≥n de consultas
QUERY_EXPANSIONS = {
    'inscripcion': ['inscribir', 'anotarse', 'anotar', 'registrar', 'inscripto'],
    'constancia': ['certificado', 'comprobante', 'papel', 'documento'],
    'regular': ['regularidad', 'condici√≥n', 'estado', 'situaci√≥n'],
    'final': ['examen', 'evaluaci√≥n', 'rendir', 'dar'],
    'recursada': ['recursar', 'volver a cursar', 'segunda vez'],
    'correlativa': ['correlatividad', 'requisito', 'necesito', 'puedo cursar'],
    'baja': ['dar de baja', 'abandonar', 'dejar', 'salir'],
}

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Salida a consola
        logging.FileHandler(Path('logs') / 'app.log')  # Salida a archivo
    ]
)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv()

# Configurar token de Hugging Face si est√° disponible
if "HUGGING_FACE_HUB_TOKEN" in os.environ:
    huggingface_hub.login(token=os.environ["HUGGING_FACE_HUB_TOKEN"], add_to_git_credential=False)
    logger.info("Configurado token de Hugging Face desde variables de entorno")

# Determinar el entorno (desarrollo o producci√≥n)
ENVIRONMENT = os.getenv('ENVIRONMENT', 'development')
logger.info(f"Iniciando sistema RAG en entorno: {ENVIRONMENT}")

# Funci√≥n para configurar el dispositivo seg√∫n preferencias
def get_device():
    """Configura el dispositivo seg√∫n preferencias y disponibilidad."""
    device_pref = os.getenv('DEVICE', 'auto')
    
    if device_pref == 'auto':
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
    else:
        device = device_pref  # Usa espec√≠ficamente cuda, cpu o mps si se especifica
    
    logger.info(f"Usando dispositivo: {device}")
    return device

# Importaciones condicionales para Pinecone (solo en producci√≥n)
if ENVIRONMENT == 'production':
    try:
        import pinecone
        PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')
        PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')
        PINECONE_INDEX_NAME = os.getenv('PINECONE_INDEX_NAME', 'uba-chatbot-embeddings')
        if not all([PINECONE_API_KEY, PINECONE_ENVIRONMENT]):
            logger.warning("Falta configuraci√≥n de Pinecone. Se usar√° FAISS local.")
            ENVIRONMENT = 'development'
        else:
            logger.info("Usando Pinecone para b√∫squeda de embeddings en producci√≥n.")
            # Inicializar pinecone
            pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)
    except ImportError:
        logger.warning("No se pudo importar pinecone. Se usar√° FAISS local.")
        ENVIRONMENT = 'development'

# Clase base abstracta para b√∫squedas vectoriales
class VectorStore:
    def search(self, query_embedding: List[float], k: int) -> List[Dict]:
        """B√∫squeda de vectores similares"""
        raise NotImplementedError("Este m√©todo debe ser implementado por las subclases")

# Implementaci√≥n para FAISS (desarrollo)
class FAISSVectorStore(VectorStore):
    def __init__(self, index_path: str, metadata_path: str):
        """
        Inicializa el almac√©n vectorial FAISS.
        
        Args:
            index_path (str): Ruta al archivo de √≠ndice FAISS
            metadata_path (str): Ruta al archivo de metadatos
        """
        if not os.path.exists(index_path):
            raise FileNotFoundError(f"No se encontr√≥ el √≠ndice FAISS en {index_path}")
            
        self.index = faiss.read_index(index_path)
        self.metadata = pd.read_csv(metadata_path)
        logger.info(f"√çndice FAISS cargado con {self.index.ntotal} vectores")
    
        # Umbral de similitud m√≠nimo (ajustable seg√∫n necesidad)
        self.similarity_threshold = 0.1  # Reducido de 0.6 a 0.1 para ser m√°s permisivo
    
    def search(self, query_embedding: List[float], k: int = 5) -> List[Dict]:
        """
        B√∫squeda de vectores similares en FAISS con mejoras.
        
        Args:
            query_embedding (List[float]): Embedding de la consulta
            k (int): N√∫mero de resultados a retornar
            
        Returns:
            List[Dict]: Lista de resultados con metadatos
        """
        # Convertir a numpy y normalizar
        query_embedding_np = np.array(query_embedding).reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding_np)
        
        # Realizar b√∫squeda
        distances, indices = self.index.search(query_embedding_np, k)
        
        # Convertir distancias L2 a similitud coseno
        similarities = np.clip(1 - distances[0] / 2, 0, 1)  # Asegurar rango [0,1]
        
        # Filtrar por umbral y ordenar por similitud
        results = []
        seen_texts = set()  # Para evitar duplicados
        
        for idx, sim in zip(indices[0], similarities):
            if idx < 0 or idx >= len(self.metadata):  # √çndice inv√°lido
                continue
            
            metadata = self.metadata.iloc[idx].to_dict()
            text = metadata.get('text', '')
            
            # Evitar duplicados y textos muy cortos
            if (text in seen_texts or 
                len(text) < 50 or 
                sim < self.similarity_threshold):
                continue
                
            seen_texts.add(text)
            metadata['similarity'] = float(sim)
            metadata['distance'] = float(distances[0][indices[0].tolist().index(idx)])
            results.append(metadata)
                
        # Ordenar por similitud
        results = sorted(results, key=lambda x: x['similarity'], reverse=True)
        
        # Logging de resultados
        if results:
            logger.info(f"Mejor similitud encontrada: {results[0]['similarity']:.3f}")
            logger.info(f"Documento m√°s relevante: {results[0].get('filename', 'N/A')}")
            logger.info(f"N√∫mero de resultados √∫nicos: {len(results)}")
        else:
            logger.warning("No se encontraron resultados que superen el umbral de similitud")
        
        return results[:k]  # Limitar a k resultados

# Implementaci√≥n para Pinecone (producci√≥n)
class PineconeVectorStore(VectorStore):
    def __init__(self, index_name: str):
        """
        Inicializa el almac√©n vectorial Pinecone.
        
        Args:
            index_name (str): Nombre del √≠ndice en Pinecone
        """
        if index_name not in pinecone.list_indexes():
            raise ValueError(f"No se encontr√≥ el √≠ndice Pinecone '{index_name}'")
            
        self.index = pinecone.Index(index_name)
        stats = self.index.describe_index_stats()
        logger.info(f"√çndice Pinecone '{index_name}' cargado con {stats['total_vector_count']} vectores")
    
    def search(self, query_embedding: List[float], k: int) -> List[Dict]:
        """
        B√∫squeda de vectores similares en Pinecone.
        
        Args:
            query_embedding (List[float]): Embedding de la consulta
            k (int): N√∫mero de resultados a retornar
            
        Returns:
            List[Dict]: Lista de resultados con metadatos
        """
        # Realizar b√∫squeda en Pinecone
        query_results = self.index.query(
            vector=query_embedding,
            top_k=k,
            include_metadata=True
        )
        
        # Formatear resultados
        results = []
        for match in query_results['matches']:
            # Extraer metadatos
            metadata = match['metadata']
            metadata['distance'] = 1.0 - match['score']  # Convertir similitud coseno a distancia
            results.append(metadata)
                
        return results

def get_available_memory_gb():
    """
    Obtiene la cantidad de memoria RAM disponible en GB.
    
    Returns:
        float: Memoria disponible en GB
    """
    try:
        # Intentar obtener la memoria virtual disponible
        available = psutil.virtual_memory().available
        available_gb = available / (1024 ** 3)  # Convertir a GB
        logger.info(f"Memoria disponible: {available_gb:.2f} GB")
        return available_gb
    except Exception as e:
        logger.warning(f"Error al obtener memoria disponible: {str(e)}")
        # En caso de error, devolver un valor que permita cargar el modelo grande
        # para no cambiar el comportamiento previo
        return 32.0

def load_model_with_fallback(model_path: str, load_kwargs: Dict) -> tuple:
    """
    Intenta cargar un modelo y, si falla por autenticaci√≥n o memoria insuficiente, 
    usa un modelo abierto como alternativa.
    Soporta optimizaciones seg√∫n plataforma.
    
    Args:
        model_path (str): Ruta al modelo preferido
        load_kwargs (Dict): Argumentos para cargar el modelo
        
    Returns:
        tuple: (modelo, tokenizer, nombre_del_modelo_cargado)
    """
    # Modelo abierto de respaldo
    fallback_model = os.getenv('FALLBACK_MODEL_NAME', 'google/gemma-2b')
    
    # Detectar si estamos en Mac con Apple Silicon
    is_mac_silicon = torch.backends.mps.is_available()
    
    # Verificar memoria disponible
    available_memory_gb = get_available_memory_gb()
    memory_threshold_gb = 16.0  # Necesitamos al menos 16GB libres para Mistral 7B
    
    # Decidir si usar el modelo de respaldo basado en la memoria disponible
    if available_memory_gb < memory_threshold_gb:
        logger.warning(f"Memoria disponible ({available_memory_gb:.2f} GB) es menor que el umbral recomendado ({memory_threshold_gb} GB)")
        logger.warning(f"Usando modelo de respaldo para evitar errores de memoria: {fallback_model}")
        model_path = fallback_model
    
    # Ajustar configuraci√≥n seg√∫n la plataforma
    if is_mac_silicon:
        logger.info("Detectado Mac con Apple Silicon, ajustando configuraci√≥n de carga")
        
        # Para Mac Silicon, desactivamos bitsandbytes independientemente de la configuraci√≥n
        # Quitamos la cuantizaci√≥n que causa problemas en Mac
        if "load_in_8bit" in load_kwargs and load_kwargs["load_in_8bit"]:
            logger.warning("Desactivando cuantizaci√≥n de 8 bits en Apple Silicon")
            load_kwargs["load_in_8bit"] = False
        if "load_in_4bit" in load_kwargs and load_kwargs["load_in_4bit"]:
            logger.warning("Desactivando cuantizaci√≥n de 4 bits en Apple Silicon")
            load_kwargs["load_in_4bit"] = False
        
        # Configurar para usar la aceleraci√≥n de MPS por defecto
        if "device_map" in load_kwargs:
            load_kwargs["device_map"] = "mps"
        
        # Aumentamos la precisi√≥n para compensar la falta de cuantizaci√≥n
        load_kwargs["torch_dtype"] = torch.float16
    else:
        # En otras plataformas (Windows/Linux), mantenemos la configuraci√≥n original
        # que puede incluir bitsandbytes si est√° activado
        logger.info("Usando configuraci√≥n est√°ndar para plataforma no-Mac")
        
        # Verificar si se ha habilitado 8bit o 4bit
        use_8bit = os.getenv('USE_8BIT', 'False').lower() == 'true'
        use_4bit = os.getenv('USE_4BIT', 'False').lower() == 'true'
        
        # Aplicar configuraci√≥n de bitsandbytes si est√° activada
        if use_8bit:
            logger.info("Activando cuantizaci√≥n de 8 bits con bitsandbytes")
            load_kwargs["load_in_8bit"] = True
        elif use_4bit:
            logger.info("Activando cuantizaci√≥n de 4 bits con bitsandbytes")
            load_kwargs["load_in_4bit"] = True
    
    try:
        logger.info(f"Intentando cargar modelo con PyTorch: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path, **load_kwargs)
        logger.info(f"Modelo cargado exitosamente con PyTorch: {model_path}")
        return model, tokenizer, model_path
    except Exception as e:
        error_str = str(e)
        if "Access to model" in error_str and "is restricted" in error_str:
            logger.warning(f"ACCESO RESTRINGIDO: El modelo {model_path} requiere autenticaci√≥n en Hugging Face.")
            logger.warning("Para usar este modelo, debes ejecutar 'huggingface-cli login' e ingresar tu token.")
            logger.warning(f"Cambiando autom√°ticamente al modelo abierto: {fallback_model}")
            
            # Cargar modelo alternativo
            tokenizer = AutoTokenizer.from_pretrained(fallback_model)
            model = AutoModelForCausalLM.from_pretrained(fallback_model, **load_kwargs)
            return model, tokenizer, fallback_model
        else:
            # Si es otro tipo de error, reenviar la excepci√≥n
            logger.error(f"Error al cargar el modelo: {error_str}")
            raise

# Clase base para modelos
class BaseModel:
    def generate(self, prompt: str, **kwargs) -> str:
        raise NotImplementedError

# Clase para usar la API de Hugging Face
class APIModel(BaseModel):
    def __init__(self, model_name: str, api_token: str, timeout: int = 30):
        """
        Inicializa el cliente de la API de Hugging Face.
        
        Args:
            model_name (str): Nombre del modelo en Hugging Face
            api_token (str): Token de la API de Hugging Face
            timeout (int): Timeout para las llamadas a la API
        """
        self.model_name = model_name
        self.api_url = f"https://api-inference.huggingface.co/models/{model_name}"
        self.headers = {"Authorization": f"Bearer {api_token}"}
        self.timeout = timeout
        
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Genera texto usando la API de Hugging Face.
        
        Args:
            prompt (str): Texto de entrada
            kwargs: Argumentos adicionales para la generaci√≥n
            
        Returns:
            str: Texto generado
        """
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": kwargs.get("max_length", 512),
                "temperature": kwargs.get("temperature", 0.7),
                "top_p": kwargs.get("top_p", 0.9),
                "top_k": kwargs.get("top_k", 50),
                "return_full_text": False
            }
        }
        
        try:
            response = requests.post(
                self.api_url, 
                headers=self.headers, 
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            result = response.json()
            
            if isinstance(result, list) and len(result) > 0:
                generated_text = result[0].get("generated_text", "")
                return generated_text.strip()
            else:
                raise ValueError(f"Respuesta inesperada de la API: {result}")
                
        except Exception as e:
            logger.error(f"Error al generar texto con la API de {self.model_name}: {str(e)}")
            raise

# Clase para modelo local
class LocalModel(BaseModel):
    def __init__(self, model, tokenizer, device):
        """
        Inicializa el modelo local.
        
        Args:
            model: Modelo de transformers
            tokenizer: Tokenizer del modelo
            device: Dispositivo para inferencia
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Genera texto usando el modelo local.
        
        Args:
            prompt (str): Texto de entrada
            kwargs: Argumentos adicionales para la generaci√≥n
            
        Returns:
            str: Texto generado
        """
        try:
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=len(inputs[0]) + kwargs.get("max_length", 512),
                    temperature=kwargs.get("temperature", 0.7),
                    top_p=kwargs.get("top_p", 0.9),
                    top_k=kwargs.get("top_k", 50),
                    pad_token_id=self.tokenizer.eos_token_id
                )
                
                return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
        except Exception as e:
            logger.error(f"Error al generar texto localmente: {str(e)}")
            raise

class RAGSystem:
    def __init__(
        self,
        model_path: str = os.getenv('BASE_MODEL_PATH', 'models/finetuned_model'),
        embeddings_dir: str = os.getenv('EMBEDDINGS_DIR', 'data/embeddings'),
        device: str = os.getenv('DEVICE', 'mps')
    ):
        """
        Inicializa el sistema RAG con la siguiente l√≥gica de prioridad:
        1. Si USE_API=False:
           - Intenta cargar modelo fine-tuneado local
           - Si falla por memoria o no existe, usa API como fallback
        2. Si USE_API=True:
           - Usa directamente la API
        """
        self.device = device
        self.embeddings_dir = Path(embeddings_dir)
        
        # Tendr√≠amos:
        self.conversation_histories = {}  # Diccionario: user_id -> historial
        self.max_history_length = 5
        
        # Obtener configuraci√≥n
        self.use_api = os.getenv('USE_API', 'True').lower() == 'true'
        self.api_token = os.getenv('HUGGING_FACE_HUB_TOKEN')
        self.base_model = os.getenv('BASE_MODEL_NAME')
        self.fallback_model = os.getenv('FALLBACK_MODEL_NAME')
        
        # Normalizar los ejemplos de intenciones para mejorar la detecci√≥n
        self.normalized_intent_examples = self._normalize_intent_examples()
        logger.info("Ejemplos de intenciones normalizados para mejorar la clasificaci√≥n")
        
        # Verificar si existe modelo fine-tuneado
        model_exists = os.path.exists(model_path) and os.path.isdir(model_path)
        
        if not self.use_api and model_exists:
            # Intentar usar modelo local primero
            try:
                available_memory = get_available_memory_gb()
                if available_memory < 16:
                    raise MemoryError(f"Memoria insuficiente ({available_memory:.2f} GB) para cargar modelo local")
                
                logger.info(f"Intentando cargar modelo fine-tuneado local desde: {model_path}")
                self.model = self._load_local_model(model_path)
                logger.info("Modelo local cargado exitosamente")
                return
            except Exception as e:
                logger.warning(f"No se pudo cargar el modelo local: {str(e)}")
                logger.info("Cambiando a API como fallback")
        
        # Si llegamos aqu√≠, usamos la API (ya sea por configuraci√≥n o como fallback)
        try:
            logger.info("Inicializando modelo via API")
            self.model = self._initialize_api_model()
        except Exception as e:
            raise RuntimeError(f"No se pudo inicializar ning√∫n modelo (local ni API): {str(e)}")
        
        # Cargar modelo de embeddings
        embedding_model_name = os.getenv('EMBEDDING_MODEL_NAME', 'hiiamsid/sentence_similarity_spanish_es')
        try:
            logger.info(f"Intentando cargar modelo de embeddings: {embedding_model_name}")
            self.embedding_model = SentenceTransformer(embedding_model_name)
            logger.info(f"Modelo de embeddings cargado: {embedding_model_name}")
            logger.info(f"Dimensi√≥n del modelo: {self.embedding_model.get_sentence_embedding_dimension()}")
        except Exception as e:
            logger.warning(f"Error al cargar modelo de embeddings principal: {str(e)}")
            logger.info("Usando modelo de respaldo con la misma dimensionalidad")
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
            logger.info(f"Dimensi√≥n del modelo de respaldo: {self.embedding_model.get_sentence_embedding_dimension()}")
        
        # Inicializar el almac√©n vectorial
        self.vector_store = self._initialize_vector_store()

        # Configurar umbral de similitud
        self.similarity_threshold = float(os.getenv('SIMILARITY_THRESHOLD', '0.1'))  # Umbral m√°s permisivo

    def _load_local_model(self, model_path: str):
        """Carga el modelo local con la configuraci√≥n apropiada"""
        load_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": "auto"
        }
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **load_kwargs
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        return LocalModel(model, tokenizer, self.device)

    def _initialize_api_model(self) -> APIModel:
        """Inicializa el modelo de API con fallback"""
        if not self.api_token:
            raise ValueError("Se requiere HUGGING_FACE_HUB_TOKEN para usar la API")
            
        try:
            # Intentar primero con Mistral
            model = APIModel(self.base_model, self.api_token)
            # Verificar que funciona
            model.generate("Test", max_length=10)
            logger.info(f"Usando modelo principal via API: {self.base_model}")
            return model
        except Exception as e:
            logger.warning(f"Error con modelo principal: {str(e)}")
            logger.info(f"Intentando con modelo de fallback: {self.fallback_model}")
            
            try:
                # Fallback a TinyLlama
                model = APIModel(self.fallback_model, self.api_token)
                # Verificar que funciona
                model.generate("Test", max_length=10)
                logger.info(f"Usando modelo de fallback via API: {self.fallback_model}")
                return model
            except Exception as e:
                raise RuntimeError(f"No se pudo inicializar ning√∫n modelo: {str(e)}")

    def _initialize_vector_store(self) -> VectorStore:
        """
        Inicializa el almac√©n vectorial adecuado seg√∫n el entorno.
        
        Returns:
            VectorStore: Implementaci√≥n del almac√©n vectorial
        """
        if ENVIRONMENT == 'production':
            # Usar Pinecone en producci√≥n
            try:
                return PineconeVectorStore(PINECONE_INDEX_NAME)
            except (ValueError, NameError) as e:
                logger.error(f"Error al inicializar Pinecone: {str(e)}")
                logger.warning("Fallback a FAISS local")
                # Fallback a FAISS si hay error
        
        # Usar FAISS en desarrollo o como fallback
        index_path = str(self.embeddings_dir / 'faiss_index.bin')
        metadata_path = str(self.embeddings_dir / 'metadata.csv')
        return FAISSVectorStore(index_path, metadata_path)
        
    def _expand_query(self, query: str) -> str:
        """
        Expande la consulta para mejorar la b√∫squeda sem√°ntica.
        """
        query_lower = query.lower()
        expanded_query = query
        
        # Palabras clave y sus expansiones
        keywords = {
            'sanci√≥n': ['sanci√≥n', 'sanciones', 'castigo', 'penalidad', 'disciplina', 'r√©gimen disciplinario'],
            'agredir': ['agredir', 'agresi√≥n', 'violencia', 'ataque', 'golpear'],
            'profesor': ['profesor', 'docente', 'maestro', 'autoridad universitaria'],
            'alumno': ['alumno', 'estudiante', 'cursante'],
            'suspensi√≥n': ['suspensi√≥n', 'expulsi√≥n', 'separaci√≥n'],
            'f√≠sicamente': ['f√≠sicamente', 'f√≠sico', 'corporal', 'material']
        }
        
        # Buscar palabras clave en la consulta
        for key, expansions in keywords.items():
            if any(word in query_lower for word in expansions):
                expanded_query = f"{expanded_query} {' '.join(expansions)}"
        
        # Agregar referencias a art√≠culos relevantes
        if any(word in query_lower for word in keywords['sanci√≥n'] + keywords['agredir']):
            expanded_query = f"{expanded_query} art√≠culo 13 art√≠culo 14 art√≠culo 15 r√©gimen disciplinario"
        
        logger.info(f"Consulta expandida: {expanded_query}")
        return expanded_query
        
    def retrieve_relevant_chunks(self, query: str, k: int = None) -> List[Dict]:
        """
        Recupera chunks relevantes para una consulta.
        
        Args:
            query (str): Consulta del usuario
            k (int): N√∫mero de chunks a recuperar
            
        Returns:
            List[Dict]: Lista de chunks relevantes con metadatos
        """
        if k is None:
            k = int(os.getenv('RAG_NUM_CHUNKS', 5))
            
        # Preprocesar la consulta
        query = query.strip()
        if not query:
            return []
            
        # Generar embedding de la consulta
        try:
            # Expandir la consulta para mejorar la b√∫squeda
            expanded_query = self._expand_query(query)
            logger.info(f"Consulta expandida: {expanded_query}")
            
            query_embedding = self.embedding_model.encode(
                [expanded_query],
                convert_to_numpy=True,
                normalize_embeddings=True
            )[0]
            logger.info(f"Embedding generado con forma: {query_embedding.shape}")
        except Exception as e:
            logger.error(f"Error al generar embedding de consulta: {str(e)}", exc_info=True)
            return []
            
        # Realizar b√∫squeda
        try:
            # Verificar que el vector_store est√° inicializado
            if not hasattr(self, 'vector_store'):
                logger.error("vector_store no est√° inicializado")
                return []
                
            logger.info("Iniciando b√∫squeda en vector_store")
            results = self.vector_store.search(query_embedding, k=k*2)
            logger.info(f"B√∫squeda completada, resultados obtenidos: {len(results)}")
            
            # Verificar si los resultados son relevantes
            if not results:
                logger.warning("No se encontraron chunks relevantes para la consulta.")
                return []
            
            # Filtrar por similitud y duplicados
            filtered_results = []
            seen_content = set()
            
            for result in results:
                text = result.get('text', '').strip()
                # Crear una versi√≥n simplificada del texto para comparaci√≥n
                simple_text = ' '.join(text.lower().split())
                
                # Verificar similitud usando el campo correcto
                similarity = result.get('similarity', 0.0)
                filename = result.get('filename', '')
                
                logger.info(f"Procesando resultado - Similitud: {similarity}, Archivo: {filename}")
                
                # Dar prioridad a chunks del r√©gimen disciplinario si la consulta es sobre sanciones
                if any(word in query.lower() for word in ['sanci√≥n', 'sanciones', 'agredir', 'agresi√≥n']):
                    if "Regimen_Disciplinario.pdf" in filename:
                        # Reducir el umbral para documentos relevantes
                        if similarity >= (self.similarity_threshold * 0.5):  # Umbral m√°s permisivo para documentos relevantes
                            if simple_text not in seen_content:
                                seen_content.add(simple_text)
                                filtered_results.append(result)
                                logger.info(f"Chunk de R√©gimen Disciplinario aceptado con similitud: {similarity:.3f}")
                    else:
                        # Mantener umbral normal para otros documentos
                        if similarity >= self.similarity_threshold and simple_text not in seen_content:
                            seen_content.add(simple_text)
                            filtered_results.append(result)
                            logger.info(f"Chunk de otro documento aceptado con similitud: {similarity:.3f}")
                else:
                    # Para otras consultas, usar el umbral normal
                    if similarity >= self.similarity_threshold and simple_text not in seen_content:
                        seen_content.add(simple_text)
                        filtered_results.append(result)
                        logger.info(f"Chunk aceptado con similitud: {similarity:.3f}")
            
            # Ordenar por similitud y limitar a k resultados
            filtered_results = sorted(filtered_results, key=lambda x: x.get('similarity', 0.0), reverse=True)[:k]
            
            # Logging de resultados
            logger.info(f"Recuperados {len(filtered_results)} chunks √∫nicos de {len(results)} totales")
            
            return filtered_results
            
        except Exception as e:
            logger.error(f"Error en la b√∫squeda de chunks: {str(e)}", exc_info=True)
            return []
        
    def _format_source_name(self, source: str) -> str:
        """
        Formatea el nombre de la fuente eliminando extensiones y caracteres especiales.
        
        Args:
            source (str): Nombre original de la fuente (ej: "Condiciones_Regularidad.pdf")
            
        Returns:
            str: Nombre formateado (ej: "Condiciones de Regularidad")
        """
        # Eliminar extensi√≥n .pdf
        source = source.replace('.pdf', '')
        
        # Reemplazar guiones bajos y guiones medios por espacios
        source = source.replace('_', ' ').replace('-', ' ')
        
        # Capitalizar palabras
        source = ' '.join(word.capitalize() for word in source.split())
        
        return source

    def generate_response(self, query: str, context: str, sources: List[str] = None) -> str:
        """
        Genera una respuesta usando el modelo de lenguaje
        """
        # Determinar el emoji seg√∫n la intenci√≥n
        intent, _ = self._get_query_intent(query)
        emoji = random.choice({
            'saludo': greeting_emojis,
            'pregunta_capacidades': information_emojis,
            'consulta_administrativa': information_emojis,
            'consulta_academica': information_emojis,
            'consulta_medica': medical_emojis,
            'consulta_reglamento': warning_emojis
        }.get(intent, information_emojis))
        
        # Construir el prompt con el contexto de la intenci√≥n y las FAQs
        intent_context = INTENT_EXAMPLES.get(intent, {}).get('context', 'Consulta general')
        
        faqs = """
[PREGUNTAS FRECUENTES]
1. Constancia de alumno regular:
   Puedes tramitar la constancia de alumno regular en el Sitio de Inscripciones siguiendo estos pasos:
   - Paso 1: Ingresar tu DNI y contrase√±a.
   - Paso 2: Seleccionar la opci√≥n "Constancia de alumno regular" en el inicio de tr√°mites.
   - Paso 3: Imprimir la constancia. Luego, deber√°s presentarte con tu Libreta Universitaria o DNI y el formulario impreso (1 hoja que posee 3 certificados de alumno regular) en la ventanilla del Ciclo Biom√©dico.

2. Baja de materia:
   El tiempo m√°ximo para dar de baja una materia es:
   - 2 semanas antes del primer parcial, o
   - Hasta el 25% de la cursada en asignaturas sin examen parcial.
   Para dar de baja una materia, sigue estos pasos en el Sitio de Inscripciones:
   - Paso 1: Ingresar tu DNI y contrase√±a.
   - Paso 2: Seleccionar "Baja de asignatura".
   - Paso 3: Imprimir el certificado de baja. Una vez finalizado el tr√°mite, el estado ser√° "Resuelto Positivamente" y no deber√°s acudir a la Direcci√≥n de Alumnos.

3. Anulaci√≥n de inscripci√≥n a final:
   Para anular la inscripci√≥n a un final, debes acudir a la ventanilla del Ciclo Biom√©dico presentando el n√∫mero de constancia generado durante el tr√°mite de inscripci√≥n.

4. No lograr inscripci√≥n o asignaci√≥n a materia:
   Si no logras inscribirte o no te asignan una materia, debes dirigirte a la c√°tedra o departamento correspondiente y solicitar la inclusi√≥n en lista, presentando tu Libreta Universitaria o DNI.

5. Reincorporaci√≥n:
   La reincorporaci√≥n se solicita a trav√©s del Sitio de Inscripciones, seleccionando la opci√≥n "Reincorporaci√≥n a la carrera":
   - Para la 1¬™ reincorporaci√≥n: El tr√°mite es autom√°tico y aparece resuelto positivamente en el sistema, sin necesidad de tr√°mite en ventanilla.
   - Si ya fuiste reincorporado anteriormente: Debes realizar el tr√°mite, imprimirlo (consta de 2 hojas: 1 certificado y 1 constancia) y presentarlo en la ventanilla del Ciclo Biom√©dico, donde la Comisi√≥n de Readmisi√≥n resolver√° tu caso.

6. Recursada (inscripci√≥n por segunda vez):
   Para solicitar una recursada, genera el tr√°mite en el Sitio de Inscripciones siguiendo estos pasos:
   - Paso 1: Ingresar tu DNI y contrase√±a.
   - Paso 2: Seleccionar "Recursada".
   El tr√°mite es autom√°tico y, si aparece resuelto positivamente en el sistema, no necesitas acudir a ventanilla.
   - Si en el sistema apareces como dado DE BAJA en la cursada anterior, solo debes generar el tr√°mite y te inscribir√°s como la primera vez, sin abonar arancel.
   - Si no apareces dado DE BAJA, deber√°s:
     1. Realizar el tr√°mite.
     2. Generar e imprimir el tal√≥n de pago.
     3. Pagar en la Direcci√≥n de Tesorer√≠a.
     4. Presentar un comprobante de pago en los buzones del Ciclo Biom√©dico.

7. Tercera cursada:
   Para solicitar la tercera cursada, sigue estos pasos en el Sitio de Inscripciones:
   - Paso 1: Ingresar tu DNI y contrase√±a.
   - Paso 2: Seleccionar "3¬∫ Cursada".
   - Paso 3: Imprimir la constancia y el certificado.
   Luego:
   - Si figuras como dado DE BAJA en las dos cursadas anteriores, te inscribes como si fuera la primera vez sin abonar arancel.
   - Si no, debes:
     1. Realizar el tr√°mite.
     2. Generar e imprimir el tal√≥n de pago.
     3. Pagar en la Direcci√≥n de Tesorer√≠a.
     4. Presentar un comprobante de pago en el buz√≥n del Ciclo Biom√©dico.

8. Cuarta cursada o m√°s:
   Para la cuarta cursada o m√°s, genera el tr√°mite en el Sitio de Inscripciones con los siguientes pasos:
   - Paso 1: Dirigirte a Inscripciones.
   - Paso 2: Ingresar tu DNI y contrase√±a.
   - Paso 3: Seleccionar "4¬∫ Cursada o m√°s".
   - Paso 4: Imprimir la constancia y el certificado.
   Luego, deber√°s presentarte con tu Libreta Universitaria y las constancias impresas en la ventanilla del Ciclo Biom√©dico y acudir a la Direcci√≥n de Alumnos.

9. Pr√≥rroga de materias:
   Para solicitar la pr√≥rroga de una asignatura, sigue estos pasos en el Sitio de Inscripciones:
   - Paso 1: Dirigirte a Inscripciones.
   - Paso 2: Ingresar tu DNI y contrase√±a.
   - Paso 3: Seleccionar "Pr√≥rroga de asignatura".
   - Paso 4: Imprimir la constancia.
   Si se trata de la primera o segunda pr√≥rroga, el tr√°mite se resuelve positivamente. Si es la tercera o una pr√≥rroga superior, deber√°s presentar la constancia impresa junto con tu Libreta Universitaria en la ventanilla del Ciclo Biom√©dico.
"""

        # Solo incluir las fuentes en el prompt si no son nulas y la lista no est√° vac√≠a
        sources_text = ""
        if sources and len(sources) > 0:
            formatted_sources = [self._format_source_name(src) for src in sources]
            sources_text = f"\nFUENTES:\n{', '.join(formatted_sources)}"

        prompt = f"""[INST]
Como DrCecim, asistente virtual de la Facultad de Medicina UBA:

CONTEXTO DE LA CONSULTA:
{intent_context}

INFORMACI√ìN RELEVANTE:
{context}

{faqs}

{sources_text}

CONSULTA:
{query}

INSTRUCCIONES:
1. Si la consulta coincide con alguna de las preguntas frecuentes, proporciona esa respuesta exacta sin mencionar fuentes
2. Si la consulta es similar pero no exacta a una pregunta frecuente, adapta la respuesta manteniendo la informaci√≥n precisa
3. Si la consulta requiere informaci√≥n de los documentos:
   - Integra la informaci√≥n naturalmente en la respuesta
   - Si es relevante mencionar la fuente, hazlo de forma natural en el contexto
   - Ejemplos de c√≥mo mencionar fuentes:
     * "Seg√∫n el Reglamento de Regularidad, ..."
     * "De acuerdo con las Condiciones de Regularidad, ..."
     * "Como establece el R√©gimen Disciplinario, ..."
4. Mant√©n el formato y la estructura de las respuestas frecuentes cuando corresponda
5. No hagas preguntas adicionales
6. Si es una consulta m√©dica, deriva al profesional
7. No agregues una secci√≥n de "Fuentes:" al final del mensaje

[/INST]"""

        response = self.model.generate(prompt)
        return f"{emoji} {response}"

    def _handle_medical_query(self) -> str:
        """
        Genera una respuesta est√°ndar para consultas m√©dicas
        """
        responses = [
            f"{random.choice(medical_emojis)} Lo siento, no puedo responder consultas m√©dicas. Por favor, consult√° con un profesional de la salud o acercate a la guardia del Hospital de Cl√≠nicas.",
            f"{random.choice(medical_emojis)} Como asistente virtual, no estoy capacitado para responder consultas m√©dicas. Te recomiendo consultar con un profesional m√©dico o acudir al Hospital de Cl√≠nicas.",
            f"{random.choice(medical_emojis)} Disculp√°, pero no puedo dar consejos m√©dicos. Para este tipo de consultas, te sugiero:\n"
            "1. Consultar con un profesional m√©dico\n"
            "2. Acudir a la guardia del Hospital de Cl√≠nicas\n"
            "3. En caso de emergencia, llamar al SAME (107)"
        ]
        return random.choice(responses)

    def _handle_outdated_info(self, response: str, source_date: str = None) -> str:
        """Manejo de informaci√≥n potencialmente desactualizada"""
        warning = "\n\n‚ö†Ô∏è Esta informaci√≥n corresponde al reglamento vigente. " \
                 "Para confirmar cualquier cambio reciente, consult√° en alumnos@fmed.uba.ar"
        return f"{response}{warning}"

    def _normalize_intent_examples(self) -> Dict:
        """
        Normaliza los ejemplos de intenciones para hacer comparaciones m√°s robustas
        """
        normalized_examples = {}
        
        for intent, data in INTENT_EXAMPLES.items():
            examples = data['examples']
            norm_examples = []
            
            for example in examples:
                # Aplicar la misma normalizaci√≥n que a las consultas
                norm_example = example.lower().strip()
                norm_example = unidecode(norm_example)  # Eliminar tildes
                norm_example = re.sub(r'[^\w\s]', '', norm_example)  # Eliminar signos de puntuaci√≥n
                norm_example = re.sub(r'\s+', ' ', norm_example).strip()  # Normalizar espacios
                norm_examples.append(norm_example)
                
            normalized_examples[intent] = {
                'examples': norm_examples,
                'context': data['context']
            }
            
        return normalized_examples

    def _get_query_intent(self, query: str) -> Tuple[str, float]:
        """
        Determina la intenci√≥n de la consulta usando similitud sem√°ntica
        """
        # Normalizaci√≥n del texto
        query_original = query
        query = query.lower().strip()
        query = unidecode(query)  # Eliminar tildes
        query = re.sub(r'[^\w\s]', '', query)  # Eliminar signos de puntuaci√≥n
        query = re.sub(r'\s+', ' ', query).strip()  # Normalizar espacios
        
        if query_original != query:
            logger.info(f"Consulta normalizada: '{query_original}' ‚Üí '{query}'")
            
        query_embedding = self.embedding_model.encode([query])[0]
        
        max_similarity = -1
        best_intent = 'desconocido'
        
        # Usar ejemplos normalizados
        for intent, data in self.normalized_intent_examples.items():
            examples = data['examples']
            example_embeddings = self.embedding_model.encode(examples)
            similarities = cosine_similarity([query_embedding], example_embeddings)[0]
            avg_similarity = np.mean(similarities)
            
            # Para debugging
            logger.debug(f"Intenci√≥n: {intent}, similitud: {avg_similarity:.2f}")
            
            if avg_similarity > max_similarity:
                max_similarity = avg_similarity
                best_intent = intent
        
        return best_intent, max_similarity

    def _generate_conversational_response(self, query: str, intent: str, user_name: str = None) -> str:
        """
        Genera una respuesta conversacional basada en la intenci√≥n detectada
        """
        context = INTENT_EXAMPLES[intent]['context'] if intent in INTENT_EXAMPLES else "Consulta general"
        
        # Determinar si es el primer mensaje o un saludo del usuario
        is_greeting = intent == 'saludo'
        is_courtesy = intent == 'cortesia'
        is_acknowledgment = intent == 'agradecimiento'
        is_capabilities = intent == 'pregunta_capacidades'
        
        # Lista de respuestas alegres para agradecimientos
        happy_responses = [
            "¬°Me alegro de haber podido ayudarte! üòä",
            "¬°Qu√© bueno que te sirvi√≥ la informaci√≥n! üåü",
            "¬°Genial! Estoy aqu√≠ para lo que necesites üí´",
            "¬°Excelente! No dudes en consultarme cualquier otra duda üéì",
            "¬°Me pone contento poder ayudarte! üòä",
            "¬°Perfecto! Seguimos en contacto üëã",
            "¬°B√°rbaro! Cualquier otra consulta, aqu√≠ estoy ü§ì"
        ]
        
        # Lista de respuestas para preguntas sobre capacidades
        capabilities_responses = [
            "Soy un asistente especializado en:\n- Tr√°mites administrativos de la facultad\n- Consultas sobre el reglamento y normativas\n- Informaci√≥n acad√©mica general\n- Procesos de inscripci√≥n y regularidad",
            "Puedo ayudarte con:\n- Tr√°mites y gestiones administrativas\n- Informaci√≥n sobre reglamentos y normativas\n- Consultas acad√©micas generales\n- Temas de inscripci√≥n y regularidad",
            "Me especializo en:\n- Asistencia con tr√°mites administrativos\n- Informaci√≥n sobre reglamentos\n- Consultas acad√©micas\n- Temas de inscripci√≥n y regularidad"
        ]
        
        # Si es un agradecimiento, devolver una respuesta alegre
        if is_acknowledgment:
            return random.choice(happy_responses)
            
        # Si es una pregunta sobre capacidades, devolver una respuesta espec√≠fica
        if is_capabilities:
            return random.choice(capabilities_responses)
        
        # Personalizar el prompt seg√∫n si tenemos el nombre del usuario
        user_context = f"El usuario se llama {user_name}. " if user_name else ""
        
        prompt = f"""[INST]
Como DrCecim, un asistente virtual de la Facultad de Medicina de la UBA:
- Usa un tono amigable y profesional
- Mant√©n las respuestas breves y directas
- No hagas preguntas adicionales
- Solo saluda si el usuario est√° saludando por primera vez
- Si conoces el nombre del usuario, √∫salo de manera natural sin forzarlo

{user_context}
Contexto de la consulta: {context}
Consulta del usuario: {query}

Instrucciones espec√≠ficas:
- Si es un saludo: {"Saluda usando el nombre del usuario si est√° disponible y menciona que puedes ayudar con tr√°mites y consultas" if is_greeting else "Responde directamente sin saludar"}
- Si es una pregunta de cortes√≠a: {"Responde amablemente mencionando el nombre si est√° disponible, pero sin volver a presentarte" if is_courtesy else "Responde directamente"}
- Si preguntan sobre tus capacidades: Explica que ayudas con tr√°mites administrativos y consultas acad√©micas
- Si es una consulta m√©dica: Explica amablemente que no puedes responder consultas m√©dicas
- Si preguntan tu identidad: Explica que eres un asistente virtual de la facultad, sin saludar nuevamente

[/INST]"""

        return self.model.generate(prompt)

    def _summarize_previous_message(self, user_id: str) -> str:
        """
        Genera un resumen del mensaje previo para un usuario.
        
        Args:
            user_id (str): ID del usuario
            
        Returns:
            str: Resumen generado
        """
        # Obtener historial del usuario
        history = self.get_user_history(user_id)
        
        if not history or len(history) < 1:
            return "No tengo un mensaje previo para resumir. ¬øPuedes hacerme una pregunta espec√≠fica?"
        
        # Obtener el √∫ltimo mensaje enviado por el bot
        last_query, last_response = history[-1]
        
        # Si el √∫ltimo mensaje es muy corto, no necesita resumen
        if len(last_response) < 150:
            return f"Mi mensaje anterior ya era bastante breve: \"{last_response}\""
        
        # Generar un resumen usando el modelo
        prompt = f"""[INST]
Como DrCecim, asistente virtual de la Facultad de Medicina UBA:

TAREA:
Genera un resumen claro y conciso de tu mensaje anterior.

MENSAJE ORIGINAL:
{last_response}

INSTRUCCIONES:
1. Resume el contenido principal manteniendo la informaci√≥n clave
2. El resumen debe ser aproximadamente 50% m√°s corto que el original
3. Mant√©n el mismo tono amigable y profesional
4. Incluye los puntos m√°s importantes y relevantes
5. Si hay pasos o instrucciones, pres√©rvales en formato de lista
6. No agregues informaci√≥n nueva que no estaba en el mensaje original
7. No uses frases como "En resumen" o "En conclusi√≥n"
[/INST]"""

        try:
            summary = self.model.generate(prompt)
            emoji = random.choice(information_emojis)
            return f"{emoji} {summary}"
        except Exception as e:
            logger.error(f"Error al generar resumen: {str(e)}")
            return "Lo siento, no pude generar un resumen en este momento. ¬øPodr√≠as hacerme una pregunta m√°s espec√≠fica?"

    def process_query(self, query: str, user_id: str = None, user_name: str = None) -> Dict[str, Any]:
        """
        Procesa una consulta utilizando el nuevo sistema de clasificaci√≥n sem√°ntica
        
        Args:
            query (str): La consulta del usuario
            user_id (str, optional): ID del usuario (n√∫mero de tel√©fono)
            user_name (str, optional): Nombre del usuario si est√° disponible
        """
        try:
            # Determinar la intenci√≥n
            intent, confidence = self._get_query_intent(query)
            logger.info(f"Intenci√≥n detectada: {intent} (confianza: {confidence:.2f})")
            
            # Si es una referencia a un mensaje anterior
            if intent == 'referencia_anterior':
                if not user_id or not self.get_user_history(user_id):
                    response = "Lo siento, no tengo mensajes previos para resumir. ¬øPuedes hacerme una pregunta espec√≠fica?"
                else:
                    response = self._summarize_previous_message(user_id)
                
                # Actualizar historial
                if user_id:
                    self.update_user_history(user_id, query, response)
                    
                return {
                    "query": query,
                    "response": response,
                    "query_type": intent,
                    "confidence": confidence,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Si es una pregunta sobre el nombre
            if intent == 'pregunta_nombre':
                if user_name:
                    # Lista de respuestas amigables sobre c√≥mo sabemos el nombre
                    name_responses = [
                        f"¬°Tu nombre es {user_name}! Lo veo en tu perfil de WhatsApp üòä",
                        f"Me aparece {user_name} en tu perfil de WhatsApp. ¬°Un gusto conocerte! üëã",
                        f"¬°Te llamas {user_name}! Lo s√© porque est√° en tu perfil de WhatsApp ü§ì",
                        f"Puedo ver que te llamas {user_name} por la info de tu perfil. ¬°Es un placer! üåü",
                        f"Tu nombre de perfil es {user_name}. ¬°As√≠ es como puedo saludarte personalmente! üòä"
                    ]
                    response = random.choice(name_responses)
                else:
                    response = "Lo siento, en este momento no puedo ver tu nombre en el perfil üòÖ"
                
                return {
                    "query": query,
                    "response": response,
                    "query_type": intent,
                    "confidence": confidence,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Si es un saludo y tenemos el nombre, personalizar la respuesta
            if intent == 'saludo' and user_name:
                greeting_response = f"{random.choice(greeting_emojis)} ¬°Hola {user_name}! Soy DrCecim, el asistente virtual de la Facultad de Medicina. ¬øEn qu√© puedo ayudarte?"
                return {
                    "query": query,
                    "response": greeting_response,
                    "query_type": intent,
                    "confidence": confidence,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Si es un agradecimiento, dar una respuesta alegre
            if intent == 'agradecimiento':
                response = self._generate_conversational_response(query, intent, user_name)
                return {
                    "query": query,
                    "response": response,
                    "query_type": intent,
                    "confidence": confidence,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Primero verificar si la consulta corresponde a una FAQ
            response = self._check_faqs(query)
            if response:
                # Si tenemos el nombre y es la primera interacci√≥n, personalizamos
                if user_name and not self.get_user_history(user_id):
                    response = f"¬°Hola {user_name}! {response}"
                
                logger.info("Consulta respondida desde FAQs")
                return {
                    "query": query,
                    "response": response,
                    "query_type": "faq",
                    "confidence": 1.0,
                    "relevant_chunks": [],
                    "sources": []  # No incluimos fuentes para FAQs
                }
            
            # Si no es una FAQ, continuar con el proceso normal
            if intent in ['saludo', 'pregunta_capacidades', 'identidad', 'cortesia']:
                response = self._generate_conversational_response(query, intent, user_name)
                return {
                    "query": query,
                    "response": response,
                    "query_type": intent,
                    "confidence": confidence,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Manejar consultas m√©dicas
            if intent == 'consulta_medica':
                return {
                    "query": query,
                    "response": self._handle_medical_query(),
                    "query_type": intent,
                    "confidence": confidence,
                    "relevant_chunks": [],
                    "sources": []
                }
            
            # Para consultas administrativas, acad√©micas y de reglamento
            # continuar con el proceso RAG normal
            # ... resto del c√≥digo existente para process_query ...

            # Establecer el n√∫mero de chunks seg√∫n el tipo de consulta
            num_chunks = {
                'reglamento': 5,  # M√°s chunks para consultas de reglamento
                'academica': 4,
                'administrativa': 3,
                'consulta_general': 3
            }.get(intent, 3)  # Default a 3 si el tipo no est√° en el diccionario
            
            logger.info(f"Procesando consulta: {query}")
            
            # Encontrar fragmentos relevantes
            relevant_chunks = self.retrieve_relevant_chunks(query, k=num_chunks)
            
            # Verificar si se encontraron chunks relevantes
            if not relevant_chunks:
                logger.warning("No se encontraron chunks relevantes para la consulta.")
                
                # Verificar si la consulta es sobre sanciones o agresiones
                if intent == 'reglamento':
                    # Intentar una nueva b√∫squeda con umbral m√°s bajo
                    logger.info("Intentando b√∫squeda espec√≠fica con umbral reducido...")
                    self.similarity_threshold = 0.1  # Reducir temporalmente el umbral
                    relevant_chunks = self.retrieve_relevant_chunks(query, k=num_chunks)
                    self.similarity_threshold = float(os.getenv('SIMILARITY_THRESHOLD', 0.3))  # Restaurar umbral
                
                if not relevant_chunks:
                    emoji = random.choice(information_emojis)
                    if intent in ['saludo', 'pregunta_capacidades', 'identidad']:
                        standard_no_info_response = f"{emoji} ¬°Hola! Lo siento, no encontr√© informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas."
                    else:
                        standard_no_info_response = f"{emoji} Lo siento, no encontr√© informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas."
                
                    return {
                        "query": query,
                        "response": standard_no_info_response,
                        "relevant_chunks": [],
                        "sources": [],
                        "query_type": intent,
                        "confidence": confidence
                    }
            
            # Construir contexto
            context_chunks = []
            sources = []
            
            for chunk in relevant_chunks:
                if "content" in chunk and chunk["content"].strip():
                    content = chunk["content"]
                elif "text" in chunk and chunk["text"].strip():
                    content = chunk["text"]
                else:
                    continue
                
                source = ""
                if "filename" in chunk and chunk["filename"]:
                    source = os.path.basename(chunk["filename"]).replace('.pdf', '')
                    if source and source not in sources:
                        sources.append(source)
                
                formatted_chunk = f"Informaci√≥n de {source}:\n{content}"
                context_chunks.append(formatted_chunk)
                logger.info(f"Agregado chunk relevante de {source}")
            
            # Unir los chunks para formar el contexto
            context = '\n\n'.join(context_chunks)
            
            if not context.strip():
                logger.warning("No se encontr√≥ contexto suficientemente relevante")
                emoji = random.choice(information_emojis)
                standard_no_info_response = f"{emoji} Lo siento, no encontr√© informaci√≥n espec√≠fica sobre esta consulta en mis documentos. Te sugiero escribir a **alumnos@fmed.uba.ar** para obtener la informaci√≥n precisa que necesitas."
                
                return {
                    "query": query,
                    "response": standard_no_info_response,
                    "relevant_chunks": [],
                    "sources": [],
                    "query_type": intent,
                    "confidence": confidence
                }
            
            logger.info(f"Se encontraron {len(context_chunks)} fragmentos relevantes de {len(sources)} fuentes")
            
            # Generar respuesta inicial
            response = self.generate_response(query, context, sources)
            
            # Verificar calidad de la respuesta
            verified_response, verification_score = self._verify_response(response, context, intent)
            logger.info(f"Verificaci√≥n de respuesta completada (score: {verification_score:.2f})")
            
            # Si la verificaci√≥n indica baja calidad, intentar regenerar
            if verification_score < 0.7:
                logger.warning("Baja calidad de respuesta detectada, intentando regenerar...")
                response = self.generate_response(query, context, sources)
                verified_response, verification_score = self._verify_response(response, context, intent)
            
            # Calcular confianza final
            final_confidence = (confidence + verification_score) / 2
            
            # Actualizar historial del usuario
            if user_id:
                self.update_user_history(user_id, query, response)
            
            return {
                "query": query,
                "response": verified_response,
                "relevant_chunks": relevant_chunks,
                "sources": sources,
                "query_type": intent,
                "confidence": final_confidence
            }
            
        except Exception as e:
            logger.error(f"Error en process_query: {str(e)}", exc_info=True)
            error_response = f"üë®‚Äç‚öïÔ∏è Lo siento, tuve un problema procesando tu consulta. Por favor, intenta de nuevo."
            return {
                "query": query,
                "response": error_response,
                "error": str(e),
                "query_type": "error",
                "confidence": 0.0
            }

    def _check_faqs(self, query: str) -> Optional[str]:
        """
        Verifica si la consulta corresponde a una FAQ y retorna la respuesta correspondiente
        """
        # Palabras clave para cada FAQ
        faq_keywords = {
            "constancia": ["constancia", "alumno regular", "certificado regular"],
            "baja": ["baja", "dar de baja", "darme de baja", "abandonar materia"],
            "anulacion": ["anular", "anulaci√≥n", "cancelar inscripci√≥n", "final"],
            "inscripcion": ["no logro inscribirme", "no salgo asignado", "no me asignan"],
            "reincorporacion": ["reincorporaci√≥n", "reincorporar", "volver a la carrera"],
            "recursada": ["recursada", "recursar", "segunda vez", "segunda cursada"],
            "tercera": ["tercera cursada", "tercera vez", "3ra cursada"],
            "cuarta": ["cuarta cursada", "cuarta vez", "4ta cursada"],
            "prorroga": ["pr√≥rroga", "prorroga", "prorrogar materia"]
        }
        
        # Normalizar la consulta
        query_normalized = unidecode(query.lower())
        
        # Buscar coincidencias
        for faq_type, keywords in faq_keywords.items():
            if any(keyword in query_normalized for keyword in keywords):
                emoji = random.choice(information_emojis)
                if faq_type == "constancia":
                    return f"{emoji} Para tramitar la constancia de alumno regular en el Sitio de Inscripciones:\n- Paso 1: Ingresar tu DNI y contrase√±a.\n- Paso 2: Seleccionar la opci√≥n \"Constancia de alumno regular\" en el inicio de tr√°mites.\n- Paso 3: Imprimir la constancia. Luego, deber√°s presentarte con tu Libreta Universitaria o DNI y el formulario impreso (1 hoja que posee 3 certificados de alumno regular) en la ventanilla del Ciclo Biom√©dico."
                elif faq_type == "recursada":
                    return f"{emoji} Para solicitar una recursada, genera el tr√°mite en el Sitio de Inscripciones siguiendo estos pasos:\n- Paso 1: Ingresar tu DNI y contrase√±a.\n- Paso 2: Seleccionar \"Recursada\".\nEl tr√°mite es autom√°tico y, si aparece resuelto positivamente en el sistema, no necesitas acudir a ventanilla.\n- Si en el sistema apareces como dado DE BAJA en la cursada anterior, solo debes generar el tr√°mite y te inscribir√°s como la primera vez, sin abonar arancel.\n- Si no apareces dado DE BAJA, deber√°s:\n  1. Realizar el tr√°mite.\n  2. Generar e imprimir el tal√≥n de pago.\n  3. Pagar en la Direcci√≥n de Tesorer√≠a.\n  4. Presentar un comprobante de pago en los buzones del Ciclo Biom√©dico."
                # ... agregar el resto de las FAQs ...
        
        return None

    def _verify_response(self, response: str, context: str, intent: str) -> tuple:
        """
        Verifica la calidad de la respuesta generada
        """
        # Inicializar el score de verificaci√≥n
        verification_score = 1.0
        
        # Verificar longitud adecuada
        if len(response) < 50:
            verification_score *= 0.7
        elif len(response) > 500:
            verification_score *= 0.8
        
        # Verificar presencia de informaci√≥n del contexto
        context_keywords = set(context.lower().split())
        response_keywords = set(response.lower().split())
        keyword_overlap = len(context_keywords.intersection(response_keywords))
        
        if keyword_overlap < 5:
            verification_score *= 0.6
        
        # Verificar formato seg√∫n tipo de consulta
        if intent == 'reglamento':
            if not any(word in response.lower() for word in ['art√≠culo', 'reglamento', 'normativa']):
                verification_score *= 0.8
        elif intent == 'administrativo':
            if not any(word in response.lower() for word in ['tr√°mite', 'pasos', 'procedimiento']):
                verification_score *= 0.8
        
        # Verificar presencia de elementos estructurales
        if not any(emoji in response for emoji in (greeting_emojis + information_emojis)):
            verification_score *= 0.9
        
        if 'fuentes consultadas' not in response.lower():
            verification_score *= 0.9
        
        return response, verification_score

    def get_user_history(self, user_id: str) -> list:
        """Obtiene el historial de un usuario espec√≠fico"""
        if user_id not in self.conversation_histories:
            self.conversation_histories[user_id] = []
        return self.conversation_histories[user_id]
    
    def update_user_history(self, user_id: str, query: str, response: str):
        """Actualiza el historial de un usuario espec√≠fico"""
        history = self.get_user_history(user_id)
        history.append((query, response))
        if len(history) > self.max_history_length:
            history = history[-self.max_history_length:]
        self.conversation_histories[user_id] = history

def main():
    """Funci√≥n principal para ejecutar el sistema RAG."""
    # Inicializar sistema RAG
    rag = RAGSystem()
    
    # Ejemplo de uso
    while True:
        query = input("\nIngrese su consulta (o 'salir' para terminar): ")
        if query.lower() == 'salir':
            break
            
        try:
            result = rag.process_query(query)
            print("\nRespuesta:", result['response'])
            for chunk in result['relevant_chunks']:
                if 'filename' in chunk and 'chunk_index' in chunk:
                    print(f"- {chunk['filename']} (chunk {chunk['chunk_index']})")
                else:
                    print(f"- {chunk.get('id', 'unknown')}")
        except Exception as e:
            logger.error(f"Error al procesar la consulta: {str(e)}")
            print("Lo siento, hubo un error al procesar tu consulta.")

if __name__ == "__main__":
    main() 